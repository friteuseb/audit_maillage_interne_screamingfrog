#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Audit de Maillage Interne - Version Compl√®te
Propose soit un nouveau crawl soit l'analyse de CSV existants
"""

import subprocess
import csv
import re
import os
import json
import time
from datetime import datetime
from collections import Counter

# Import de l'analyseur s√©mantique
try:
    from ext_analyseur_semantique import get_semantic_analyzer
    SEMANTIC_ANALYSIS_AVAILABLE = True
except ImportError:
    SEMANTIC_ANALYSIS_AVAILABLE = False
from urllib.parse import urlparse
import glob

class CompleteLinkAuditor:
    def __init__(self, config_file='ext_configuration_audit.json'):
        self.config = self.load_config(config_file)
        
    def load_config(self, config_file):
        """Charge la configuration"""
        import platform
        import os

        # D√©tecter le syst√®me d'exploitation pour le chemin par d√©faut
        system = platform.system()
        if system == "Windows":
            possible_paths = [
                "C:\\Program Files\\Screaming Frog SEO Spider\\ScreamingFrogSEOSpiderCli.exe",
                "C:\\Program Files (x86)\\Screaming Frog SEO Spider\\ScreamingFrogSEOSpiderCli.exe",
                "C:\\Screaming Frog\\ScreamingFrogSEOSpiderCli.exe"
            ]
            default_sf_path = next((path for path in possible_paths if os.path.exists(path)), possible_paths[0])
        elif system == "Darwin":  # macOS
            possible_paths = [
                "/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderCli",
                "/usr/local/bin/screamingfrogseospider",
                "/opt/homebrew/bin/screamingfrogseospider"
            ]
            default_sf_path = next((path for path in possible_paths if os.path.exists(path)), possible_paths[0])
        else:  # Linux et autres
            possible_paths = [
                "/usr/bin/screamingfrogseospider",
                "/usr/local/bin/screamingfrogseospider",
                "/opt/screamingfrog/screamingfrogseospider"
            ]
            default_sf_path = next((path for path in possible_paths if os.path.exists(path)), possible_paths[0])

        default_config = {
            "screaming_frog_path": default_sf_path,
            "export_path": "./exports/",
            "ignore_extensions": [".pdf", ".jpg", ".png", ".gif", ".css", ".js", ".ico", ".svg"],
            "min_anchor_length": 3
        }
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Config file {config_file} not found, using defaults")
            
        return default_config

    def show_menu(self):
        """Affiche le menu principal"""
        print("üîó AUDIT DE MAILLAGE INTERNE")
        print("="*50)
        print("Choisissez une option :")
        print()
        print("1. üï∑Ô∏è  Nouveau crawl Screaming Frog + Analyse")
        print("2. üìä Analyser un CSV existant")
        print("3. üìÅ Lister les CSV disponibles")
        print("4. ‚öôÔ∏è  Configuration")
        print("5. ‚ùå Quitter")
        print()
        
        while True:
            choice = input("Votre choix (1-5): ").strip()
            if choice in ['1', '2', '3', '4', '5']:
                return choice
            print("‚ùå Choix invalide, veuillez choisir entre 1 et 5")

    def list_existing_csvs(self):
        """Liste les CSV disponibles"""
        csv_files = []
        
        # Chercher dans le dossier exports
        if os.path.exists(self.config['export_path']):
            csv_files.extend(glob.glob(f"{self.config['export_path']}*.csv"))
        
        # Chercher dans le r√©pertoire courant
        csv_files.extend(glob.glob("*.csv"))
        
        # Supprimer les doublons et trier par date de modification
        csv_files = list(set(csv_files))
        csv_files.sort(key=os.path.getmtime, reverse=True)
        
        if not csv_files:
            print("üì≠ Aucun fichier CSV trouv√©")
            return []
        
        print(f"\nüìÅ Fichiers CSV disponibles ({len(csv_files)}):")
        print("-" * 60)
        
        for i, file_path in enumerate(csv_files, 1):
            file_size = os.path.getsize(file_path)
            file_date = datetime.fromtimestamp(os.path.getmtime(file_path))
            print(f"{i:2d}. {os.path.basename(file_path)}")
            print(f"    üìç {file_path}")
            print(f"    üìä {file_size:,} octets | üìÖ {file_date.strftime('%d/%m/%Y %H:%M')}")
            print()
        
        return csv_files

    def create_screaming_frog_config(self, user_agent=None, attempt_number=1):
        """Cr√©e un fichier de configuration temporaire pour Screaming Frog"""
        import tempfile
        import json as json_module

        # Liste de user-agents pour diff√©rentes tentatives
        fallback_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
            "Screaming Frog SEO Spider/18.0"  # Dernier recours avec l'agent par d√©faut
        ]

        if user_agent is None:
            # Utiliser l'agent de la config ou un agent de fallback
            if attempt_number == 1:
                user_agent = self.config.get('crawl_settings', {}).get('user_agent', fallback_user_agents[0])
            else:
                # Essayer diff√©rents agents selon le nombre de tentatives
                agent_index = min(attempt_number - 1, len(fallback_user_agents) - 1)
                user_agent = fallback_user_agents[agent_index]

        print(f"üîß Tentative {attempt_number}: User-Agent = {user_agent}")

        # Configuration Screaming Frog en format JSON
        config_data = {
            "spider": {
                "general": {
                    "userAgent": {
                        "userAgent": user_agent,
                        "robotsUserAgent": "Googlebot"  # Pour suivre les directives robots.txt de Googlebot
                    },
                    "crawlDelay": self.config.get('crawl_settings', {}).get('crawl_delay', 0.5),
                    "respectRobots": self.config.get('crawl_settings', {}).get('respect_robots', True),
                    "maxCrawlDepth": self.config.get('crawl_settings', {}).get('crawl_depth', 10)
                },
                "limits": {
                    "maxPages": 10000  # Limite par d√©faut pour √©viter des crawls trop longs
                }
            }
        }

        # Cr√©er un fichier temporaire pour la configuration
        temp_config = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.json', prefix='sf_config_')
        try:
            json_module.dump(config_data, temp_config, indent=2)
            temp_config.flush()
            print(f"üìÅ Config temporaire cr√©√©e: {temp_config.name}")
            return temp_config.name
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la cr√©ation de la config: {e}")
            return None
        finally:
            temp_config.close()

    def run_new_crawl(self, website_url, url_filter=None, max_attempts=3):
        """Lance un nouveau crawl Screaming Frog avec diagnostic"""
        print(f"\nüöÄ NOUVEAU CRAWL")
        print("="*50)
        print(f"Site: {website_url}")
        
        # V√©rifier que Screaming Frog existe
        sf_path = self.config["screaming_frog_path"]
        if not os.path.exists(sf_path):
            print(f"‚ùå Screaming Frog non trouv√© √†: {sf_path}")
            print("\nüí° Solutions possibles:")
            print("1. Installer Screaming Frog SEO Spider")
            print("2. Mettre √† jour le chemin dans ext_configuration_audit.json")
            print("3. Utiliser l'option 2 pour analyser un CSV existant")
            
            # Chemins alternatifs courants
            alt_paths = [
                "/mnt/c/Program Files/Screaming Frog SEO Spider/ScreamingFrogSEOSpiderCli.exe",
                "/mnt/c/Program Files (x86)/Screaming Frog SEO Spider/ScreamingFrogSEOSpiderCli.exe",
                "C:\\Program Files\\Screaming Frog SEO Spider\\ScreamingFrogSEOSpiderCli.exe",
                "C:\\Program Files (x86)\\Screaming Frog SEO Spider\\ScreamingFrogSEOSpiderCli.exe"
            ]
            
            print("\nüîç Chemins alternatifs √† essayer:")
            for path in alt_paths:
                if os.path.exists(path):
                    print(f"  ‚úÖ Trouv√©: {path}")
                else:
                    print(f"  ‚ùå Absent: {path}")
            
            return None
        
        # Cr√©er le dossier d'export
        os.makedirs(self.config['export_path'], exist_ok=True)
        
        # Tester la connectivit√© r√©seau
        print("üåê Test de connectivit√©...")
        try:
            from urllib.request import urlopen
            from urllib.parse import urlparse
            
            # Test simple de connectivit√©
            parsed = urlparse(website_url)
            test_url = f"{parsed.scheme}://{parsed.netloc}"
            
            response = urlopen(test_url, timeout=10)
            if response.status == 200:
                print("‚úÖ Site accessible")
            else:
                print(f"‚ö†Ô∏è  R√©ponse HTTP {response.status}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Probl√®me de connectivit√©: {e}")
            print("üí° V√©rifiez votre connexion internet et l'URL du site")
        
        # Commande Screaming Frog - retour √† la version qui fonctionnait
        # Cr√©er le dossier d'export local et nettoyer les anciens fichiers
        os.makedirs(self.config['export_path'], exist_ok=True)
        
        # Nettoyer les anciens fichiers CSV pour √©viter les conflits
        import glob
        old_csvs = glob.glob(f"{self.config['export_path']}*.csv")
        for old_csv in old_csvs:
            try:
                os.remove(old_csv)
                print(f"üóëÔ∏è  Supprim√©: {os.path.basename(old_csv)}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Impossible de supprimer {old_csv}: {e}")
        
        # Utiliser le chemin absolu pour le dossier d'export
        output_folder = os.path.abspath(self.config['export_path'])
        print(f"üîç Chemin d'export utilis√©: '{output_folder}'")
        
        # V√©rifier si les fonctionnalit√©s s√©mantiques sont disponibles
        semantic_exports = ""
        if self.config.get('enable_semantic_analysis', False):
            semantic_exports = ",Embeddings:All,Content Clusters:Similar Pages"
            print("üß† Analyse s√©mantique activ√©e (n√©cessite SF v22+ et API AI configur√©e)")

        # Essayer diff√©rentes configurations en cas d'√©chec
        for attempt in range(1, max_attempts + 1):
            print(f"\nüîÑ Tentative {attempt}/{max_attempts}")

            # Cr√©er une configuration temporaire avec user-agent personnalis√©
            config_file = self.create_screaming_frog_config(attempt_number=attempt)
            if not config_file:
                continue

            crawl_command = [
                sf_path,
                "--headless",
                "--crawl", website_url,
                "--config", config_file,
                "--output-folder", output_folder,
                "--export-format", "csv",
                "--bulk-export", f"All Outlinks,All Inlinks{semantic_exports}"
            ]

            print(f"üîß Commande compl√®te: {' '.join(crawl_command[:6])} [...]")
            print(f"üîß Config utilis√©e: {config_file}")

            try:
                print("‚è≥ Crawl en cours...")
                print("üí° Monitoring en temps r√©el :")

                # Lancer le processus sans capturer la sortie pour voir le feedback
                process = subprocess.Popen(
                    crawl_command,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=False,  # Mode binaire pour g√©rer l'encodage manuellement
                    bufsize=1
                )

                # Afficher la sortie en temps r√©el
                output_lines = []
                start_time = time.time()
                last_update = start_time
                initial_files = set(os.listdir(output_folder)) if os.path.exists(output_folder) else set()

                print("üìù Logs Screaming Frog:")
                print("-" * 50)

                while True:
                    raw_line = process.stdout.readline()
                    if not raw_line and process.poll() is not None:
                        break

                    # G√©rer l'encodage avec plusieurs tentatives
                    line = ""
                    for encoding in ['utf-8', 'latin-1', 'cp1252', 'ascii']:
                        try:
                            line = raw_line.decode(encoding).strip()
                            break
                        except UnicodeDecodeError:
                            continue

                    if line:
                        output_lines.append(line)
                        # Afficher seulement les lignes importantes
                        if any(keyword in line.lower() for keyword in [
                            'crawling', 'found', 'discovered', 'completed', 'error',
                            'finished', 'exported', 'links', 'pages', 'progress', 'update'
                        ]):
                            print(f"üìã {line}")

                    # Afficher un indicateur de progression toutes les 30 secondes
                    current_time = time.time()
                    if current_time - last_update > 30:
                        elapsed = int(current_time - start_time)

                        # V√©rifier les nouveaux fichiers cr√©√©s
                        if os.path.exists(output_folder):
                            current_files = set(os.listdir(output_folder))
                            new_files = current_files - initial_files
                            if new_files:
                                print(f"üìÅ Nouveaux fichiers: {', '.join(new_files)}")

                        print(f"‚è±Ô∏è  Temps √©coul√©: {elapsed//60}m {elapsed%60}s - Crawl en cours...")
                        last_update = current_time

                # Attendre la fin du processus
                return_code = process.wait()
                total_time = int(time.time() - start_time)

                print("-" * 50)
                print(f"‚è±Ô∏è  Dur√©e totale: {total_time//60}m {total_time%60}s")

                # Cr√©er un objet result compatible
                class MockResult:
                    def __init__(self, returncode, stdout_lines):
                        self.returncode = returncode
                        self.stdout = '\n'.join(stdout_lines)
                        self.stderr = ''

                result = MockResult(return_code, output_lines)

                # Diagnostic d√©taill√©
                if result.returncode == 0:
                    print("‚úÖ Crawl termin√© avec succ√®s")

                    # Chercher le fichier CSV cr√©√© dans le dossier d'export
                    csv_files = []
                    if os.path.exists(self.config['export_path']):
                        all_files = os.listdir(self.config['export_path'])
                        csv_files = [f for f in all_files if f.endswith('.csv') and ('outlink' in f.lower() or 'liens_sortants' in f.lower())]

                        print(f"üìÅ Fichiers cr√©√©s dans {self.config['export_path']}:")
                        for f in all_files:
                            print(f"  - {f}")

                    if csv_files:
                        latest_file = max([f"{self.config['export_path']}{f}" for f in csv_files], key=lambda x: os.path.getctime(x))
                        print(f"üìÑ Fichier CSV des liens: {latest_file}")
                        # Nettoyer le fichier de config temporaire
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        return latest_file
                    else:
                        print("‚ö†Ô∏è  Fichier CSV des liens non trouv√©")
                        print("üí° Le crawl a peut-√™tre √©chou√© ou aucun lien trouv√©")
                        # Nettoyer le fichier de config temporaire
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        return None
                else:
                    print(f"‚ùå Erreur lors du crawl (code: {result.returncode})")

                    # Analyser les logs pour identifier les probl√®mes
                    output_text = result.stdout.lower()

                    print(f"\nüîç DEBUG - Code de retour: {result.returncode}")
                    print(f"üîç DEBUG - Taille de sortie: {len(result.stdout)} caract√®res")
                
                # Rechercher des indices dans les logs
                if "403" in output_text or "forbidden" in output_text:
                    print("üö´ Erreur 403 Forbidden d√©tect√©e")
                    if attempt < max_attempts:
                        print(f"üîÑ Tentative avec un autre User-Agent ({attempt + 1}/{max_attempts})")
                        # Nettoyer le fichier de config temporaire
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        continue  # Passer √† la tentative suivante
                    else:
                        print("‚ùå Toutes les tentatives ont √©chou√©")
                        print("üí° Solutions possibles :")
                        print("   - Le site bloque tous les types de crawlers/bots")
                        print("   - V√©rifier si le site n√©cessite une authentification")
                        print("   - Essayer avec une limite de vitesse plus lente")
                        print("   - Contacter l'administrateur du site")
                elif "404" in output_text or "not found" in output_text:
                    print("üîç Erreur 404 - URL non trouv√©e")
                    print("üí° V√©rifiez que l'URL de d√©part existe bien")
                    # Cette erreur ne justifie pas une nouvelle tentative
                    break
                    
                    # Test rapide de connectivit√© depuis WSL
                    try:
                        test_result = subprocess.run(['curl', '-I', '-s', '--max-time', '10', website_url], 
                                                   capture_output=True, text=True, timeout=15)
                        if test_result.returncode == 0 and '200' in test_result.stdout:
                            print("‚úÖ L'URL est accessible depuis WSL - probl√®me avec Screaming Frog")
                            print("üí° Essayez de red√©marrer Screaming Frog ou v√©rifiez les param√®tres r√©seau")
                        else:
                            print("‚ùå L'URL n'est pas accessible depuis WSL non plus")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Impossible de tester la connectivit√©: {e}")
                elif "timeout" in output_text or "connection" in output_text:
                    print("‚è∞ Probl√®me de connexion/timeout")
                    if attempt < max_attempts:
                        print(f"üîÑ Nouvelle tentative avec d√©lai plus long ({attempt + 1}/{max_attempts})")
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        time.sleep(5)  # Attendre un peu avant la nouvelle tentative
                        continue
                    else:
                        print("üí° Le site met trop de temps √† r√©pondre de fa√ßon r√©p√©t√©e")
                elif "license" in output_text:
                    print("üìÑ Probl√®me de licence Screaming Frog")
                    print("üí° V√©rifiez votre licence ou utilisez la version gratuite")
                    break  # Pas de tentative suppl√©mentaire pour un probl√®me de licence
                elif "memory" in output_text or "heap" in output_text:
                    print("üíæ Probl√®me de m√©moire")
                    print("üí° Augmentez la m√©moire allou√©e √† Screaming Frog")
                    break  # Pas de tentative suppl√©mentaire pour un probl√®me de m√©moire
                elif "rate limit" in output_text or "too many requests" in output_text:
                    print("üö¶ Limite de taux d√©tect√©e")
                    if attempt < max_attempts:
                        wait_time = attempt * 10  # Attendre de plus en plus longtemps
                        print(f"üîÑ Attente de {wait_time}s avant la tentative {attempt + 1}/{max_attempts}")
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        time.sleep(wait_time)
                        continue
                    else:
                        print("üí° Le site applique une limite de taux tr√®s stricte")
                elif "blocked" in output_text or "banned" in output_text:
                    print("üö´ Site bloquant activement les requ√™tes")
                    if attempt < max_attempts:
                        print(f"üîÑ Tentative avec un User-Agent diff√©rent ({attempt + 1}/{max_attempts})")
                        if config_file and os.path.exists(config_file):
                            os.unlink(config_file)
                        continue
                    else:
                        print("üí° Le site bloque syst√©matiquement les tentatives d'acc√®s")
                
                # Afficher une partie des logs pour diagnostic
                if result.stdout:
                    print(f"\nüì§ Derniers logs (500 caract√®res):")
                    print(result.stdout[-500:])
                
                # V√©rifier si des fichiers ont quand m√™me √©t√© cr√©√©s
                if os.path.exists(output_folder):
                    current_files = set(os.listdir(output_folder))
                    new_files = current_files - initial_files
                    if new_files:
                        print(f"\nüìÅ Fichiers cr√©√©s malgr√© l'erreur: {', '.join(new_files)}")
                        # Essayer de continuer avec les fichiers partiels
                        csv_files = [f for f in new_files if f.endswith('.csv') and ('outlink' in f.lower() or 'liens_sortants' in f.lower())]
                        if csv_files:
                            latest_file = f"{output_folder}/{csv_files[0]}"
                            print(f"‚ö†Ô∏è  Tentative d'analyse du fichier partiel: {latest_file}")
                            return latest_file
                
                # Nettoyer le fichier de config temporaire
                if config_file and os.path.exists(config_file):
                    os.unlink(config_file)

            except subprocess.TimeoutExpired:
                print("‚è∞ Crawl interrompu (timeout apr√®s 1h)")
                print("üí° Le site est peut-√™tre trop volumineux, essayez avec une limite de pages")
                # Nettoyer le fichier de config temporaire
                if config_file and os.path.exists(config_file):
                    os.unlink(config_file)
                if attempt < max_attempts:
                    print(f"üîÑ Nouvelle tentative avec timeout ({attempt + 1}/{max_attempts})")
                    continue
                return None

            except FileNotFoundError:
                print(f"‚ùå Ex√©cutable non trouv√©: {sf_path}")
                print("üí° V√©rifiez l'installation et le chemin de Screaming Frog")
                # Nettoyer le fichier de config temporaire
                if config_file and os.path.exists(config_file):
                    os.unlink(config_file)
                return None

            except Exception as e:
                print(f"‚ùå Erreur inattendue lors de la tentative {attempt}: {e}")
                # Nettoyer le fichier de config temporaire
                if config_file and os.path.exists(config_file):
                    os.unlink(config_file)
                if attempt < max_attempts:
                    print(f"üîÑ Nouvelle tentative ({attempt + 1}/{max_attempts})")
                    continue
                return None

            # Si on arrive ici avec succ√®s, nettoyer le fichier de config et retourner
            if config_file and os.path.exists(config_file):
                os.unlink(config_file)
            break  # Sortir de la boucle de retry en cas de succ√®s

        # Si toutes les tentatives ont √©chou√©
        print("‚ùå Toutes les tentatives de crawl ont √©chou√©")
        return None

    def load_csv_file(self, csv_path):
        """Charge un fichier CSV avec gestion d'erreur robuste"""
        if not os.path.exists(csv_path):
            print(f"‚ùå Fichier non trouv√©: {csv_path}")
            return None, None
            
        print(f"üìÅ Chargement de: {os.path.basename(csv_path)}")
        
        # V√©rifier la taille du fichier
        try:
            file_size = os.path.getsize(csv_path)
            if file_size == 0:
                print("‚ùå Le fichier CSV est vide")
                return None, None
            elif file_size > 100 * 1024 * 1024:  # 100MB
                print(f"‚ö†Ô∏è  Fichier volumineux ({file_size // 1024 // 1024}MB), le traitement peut √™tre lent")
        except OSError as e:
            print(f"‚ùå Erreur lors de la lecture du fichier: {e}")
            return None, None
        
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        last_error = None
        
        for encoding in encodings:
            try:
                with open(csv_path, 'r', encoding=encoding, newline='') as f:
                    # D√©tecter le d√©limiteur avec une analyse plus robuste
                    sample = f.read(2048)
                    f.seek(0)
                    
                    if not sample.strip():
                        print("‚ùå Le fichier CSV est vide ou ne contient que des espaces")
                        return None, None
                    
                    # D√©tection intelligente du d√©limiteur
                    delimiter = ','
                    tab_count = sample.count('\t')
                    comma_count = sample.count(',')
                    semicolon_count = sample.count(';')
                    
                    if tab_count > comma_count and tab_count > semicolon_count:
                        delimiter = '\t'
                    elif semicolon_count > comma_count:
                        delimiter = ';'
                    
                    reader = csv.DictReader(f, delimiter=delimiter)
                    
                    # V√©rifier que nous avons des colonnes
                    if not reader.fieldnames:
                        continue
                    
                    # Nettoyer les noms de colonnes (BOM, espaces)
                    clean_fieldnames = []
                    for field in reader.fieldnames:
                        if field:
                            clean_field = field.strip().strip('\ufeff"\'')
                            clean_fieldnames.append(clean_field)
                    
                    if not clean_fieldnames:
                        continue
                    
                    # Charger les donn√©es avec validation
                    rows = []
                    for i, row in enumerate(reader):
                        if i > 500000:  # Limite de s√©curit√©
                            print(f"‚ö†Ô∏è  Limitation √† 500,000 lignes pour √©viter les probl√®mes de m√©moire")
                            break
                        
                        # Cr√©er un dictionnaire avec les noms nettoy√©s
                        clean_row = {}
                        for old_key, new_key in zip(reader.fieldnames, clean_fieldnames):
                            clean_row[new_key] = row.get(old_key, '')
                        rows.append(clean_row)
                    
                    if not rows:
                        print("‚ùå Aucune donn√©e trouv√©e dans le fichier CSV")
                        return None, None
                    
                print(f"‚úÖ Fichier charg√© avec l'encodage {encoding} ({len(rows):,} lignes)")
                print(f"üìã Colonnes: {', '.join(clean_fieldnames[:5])}{'...' if len(clean_fieldnames) > 5 else ''}")
                return rows, clean_fieldnames
                
            except UnicodeDecodeError as e:
                last_error = f"Erreur d'encodage {encoding}: {e}"
                continue
            except csv.Error as e:
                last_error = f"Erreur CSV avec {encoding}: {e}"
                continue
            except Exception as e:
                last_error = f"Erreur inattendue avec {encoding}: {e}"
                continue
                
        print(f"‚ùå Impossible de charger le fichier CSV. Derni√®re erreur: {last_error}")
        return None, None

    def is_internal_link(self, source_url, dest_url):
        """V√©rifie si le lien est interne"""
        try:
            source_domain = urlparse(source_url).netloc
            dest_domain = urlparse(dest_url).netloc
            return source_domain == dest_domain
        except:
            return False

    def is_mechanical_link(self, row):
        """D√©termine si un lien est m√©canique avec d√©tection avanc√©e"""
        # G√©rer les diff√©rents noms de colonnes possibles
        anchor_col = None
        origin_col = None
        xpath_col = None
        link_type_col = None
        
        for col in row.keys():
            col_lower = col.lower().strip('\ufeff"')
            if 'anchor' in col_lower or 'ancrage' in col_lower:
                anchor_col = col
            elif 'origin' in col_lower or 'origine' in col_lower:
                origin_col = col
            elif 'xpath' in col_lower or 'chemin' in col_lower:
                xpath_col = col
            elif 'type' in col_lower and 'link' in col_lower:
                link_type_col = col
        
        anchor = str(row.get(anchor_col, '')).lower().strip() if anchor_col else ''
        origin = str(row.get(origin_col, '')).lower() if origin_col else ''
        xpath = str(row.get(xpath_col, '')).lower() if xpath_col else ''
        link_type = str(row.get(link_type_col, '')).lower() if link_type_col else ''
        
        # Utiliser la configuration pour les patterns
        mechanical_patterns = self.config.get('mechanical_anchor_patterns', [])
        mechanical_selectors = self.config.get('mechanical_selectors', [])
        
        # 1. Liens de navigation d√©tect√©s par Screaming Frog
        navigation_origins = ['navigation', 'en-t√™te', 'pied de page', 'header', 'footer', 'nav', 'menu']
        if any(x in origin for x in navigation_origins):
            return True
        
        # 2. Type de lien explicite (si disponible)
        if link_type in ['navigation', 'menu', 'footer', 'header', 'breadcrumb']:
            return True
        
        # 3. Patterns d'ancres m√©caniques (utilise la config)
        default_mechanical_anchors = [
            r'^(accueil|home|menu|navigation)$',
            r'^(suivant|pr√©c√©dent|next|previous|page \d+)$',
            r'^(lire la suite|en savoir plus|voir plus|read more|more)$',
            r'^(contact|√† propos|mentions l√©gales|cgv|politique|privacy)$',
            r'^\d+$',  # Seulement des chiffres
            r'^(cliquez ici|cliquer ici|ici|click here|here)$',
            r'^(retour|back|retour accueil)$',
            r'^(passer au contenu|skip to content)$',
            r'^(voir tout|see all|view all)$',
            r'^(\+|\-|\>|\<|\¬ª|\¬´)$',  # Symboles seuls
            r'^$'  # Ancres vides
        ]
        
        patterns_to_use = mechanical_patterns if mechanical_patterns else default_mechanical_anchors
        
        for pattern in patterns_to_use:
            if re.search(pattern, anchor, re.IGNORECASE):
                return True
        
        # 4. D√©tection avanc√©e par position/contexte XPath
        mechanical_xpath_patterns = [
            r'(header|footer|nav|navigation|menu)',
            r'(breadcrumb|pagination|pager)',
            r'(sidebar|widget|aside)',
            r'(social|share|partage)',
            r'(tag|category|categorie)',
            r'(related|connexe|similaire)',
            r'role=["\']?(navigation|menu|banner|contentinfo)["\']?'
        ]
        
        for pattern in mechanical_xpath_patterns:
            if re.search(pattern, xpath, re.IGNORECASE):
                return True
        
        # 5. D√©tection par s√©lecteurs CSS (si disponible dans xpath)
        default_selectors = ['.menu', '.nav', '.header', '.footer', '.breadcrumb', '.pagination', '.sidebar']
        selectors_to_use = mechanical_selectors if mechanical_selectors else default_selectors
        
        for selector in selectors_to_use:
            if selector.replace('.', '') in xpath or selector in xpath:
                return True
        
        # 6. Ancres tr√®s courtes ou non descriptives
        if len(anchor.strip()) <= 2 and anchor.strip() not in ['tv', 'pc', 'seo', 'api', 'faq']:
            return True
        
        # 7. D√©tection d'URL compl√®tes comme ancres (souvent m√©caniques)
        if anchor.startswith(('http://', 'https://', 'www.')):
            return True
            
        return False

    def matches_url_filter(self, row, url_filter, column_mapping):
        """V√©rifie si un lien correspond au filtre d'URL sp√©cifi√©"""
        source_col = column_mapping.get('source')
        dest_col = column_mapping.get('dest')
        
        if not source_col or not dest_col:
            return True  # Si pas de colonnes, garder tout
        
        source = row.get(source_col, '').strip()
        dest = row.get(dest_col, '').strip()
        
        # Garder le lien si la source OU la destination correspondent au filtre
        source_matches = source.startswith(url_filter) if source else False
        dest_matches = dest.startswith(url_filter) if dest else False
        
        return source_matches or dest_matches

    def analyze_csv(self, csv_path, website_url=None, url_filter=None):
        """Analyse un fichier CSV avec gestion d'erreur compl√®te"""
        print(f"\nüìä ANALYSE DU FICHIER CSV")
        print("="*50)
        
        try:
            # Charger le CSV
            rows, fieldnames = self.load_csv_file(csv_path)
            if not rows:
                print("‚ùå Impossible de continuer sans donn√©es")
                return None
            
            # D√©tecter l'URL du site si pas fournie
            if not website_url and rows:
                try:
                    first_source = rows[0].get('Source', '') or rows[0].get('source', '') or rows[0].get('URL', '')
                    if first_source:
                        parsed = urlparse(first_source)
                        if parsed.scheme and parsed.netloc:
                            website_url = f"{parsed.scheme}://{parsed.netloc}/"
                        else:
                            print("‚ö†Ô∏è  URL du site non d√©tect√©e automatiquement")
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur lors de la d√©tection de l'URL: {e}")
            
            if not website_url:
                print("‚ö†Ô∏è  URL du site manquante - certaines analyses peuvent √™tre limit√©es")
            else:
                print(f"üåê Site analys√©: {website_url}")
                
            # Identifier les colonnes importantes AVANT le filtrage
            column_mapping = self.identify_columns(fieldnames)
            if not column_mapping['source'] or not column_mapping['dest']:
                print("‚ùå Colonnes Source/Destination non trouv√©es")
                print(f"üìã Colonnes disponibles: {', '.join(fieldnames)}")
                return None
            
            if url_filter:
                print(f"üéØ Filtrage activ√©: {url_filter}")
                # Filtrer les lignes selon le pr√©fixe d'URL
                original_count = len(rows)
                rows = [row for row in rows if self.matches_url_filter(row, url_filter, column_mapping)]
                filtered_count = len(rows)
                print(f"üìä Filtrage: {original_count:,} ‚Üí {filtered_count:,} liens ({original_count-filtered_count:,} supprim√©s)")
                
                if filtered_count == 0:
                    print("‚ùå Aucun lien ne correspond au filtre sp√©cifi√©")
                    return None
            
            print(f"üìã Colonnes utilis√©es:")
            print(f"  - Source: {column_mapping['source']}")
            print(f"  - Destination: {column_mapping['dest']}")
            print(f"  - Ancre: {column_mapping['anchor'] or 'Non trouv√©e'}")
            
            # Analyser les liens
            internal_links = []
            external_links = []
            mechanical_count = 0
            editorial_count = 0
            error_count = 0
            
            for i, row in enumerate(rows):
                try:
                    source = row.get(column_mapping['source'], '').strip()
                    destination = row.get(column_mapping['dest'], '').strip()
                    
                    # Valider les URLs
                    if not source or not destination:
                        error_count += 1
                        continue
                    
                    # Filtrer les liens internes
                    if self.is_internal_link(source, destination):
                        internal_links.append(row)
                        
                        # Classifier m√©canique vs √©ditorial
                        if self.is_mechanical_link(row):
                            mechanical_count += 1
                            row['is_mechanical'] = True
                        else:
                            editorial_count += 1
                            row['is_mechanical'] = False
                    else:
                        external_links.append(row)
                        
                except Exception as e:
                    error_count += 1
                    if error_count <= 5:  # Afficher seulement les 5 premi√®res erreurs
                        print(f"‚ö†Ô∏è  Erreur ligne {i+2}: {e}")
            
            if error_count > 5:
                print(f"‚ö†Ô∏è  ... et {error_count - 5} autres erreurs ignor√©es")
            
            print(f"\nüìä R√©sultats:")
            print(f"  üìà Liens totaux: {len(rows):,}")
            print(f"  üè† Liens internes: {len(internal_links):,}")
            print(f"  üåç Liens externes: {len(external_links):,}")
            print(f"  üîß Liens m√©caniques: {mechanical_count:,}")
            print(f"  ‚úçÔ∏è  Liens √©ditoriaux: {editorial_count:,}")
            
            if error_count > 0:
                print(f"  ‚ö†Ô∏è  Erreurs ignor√©es: {error_count:,}")
            
            if len(internal_links) > 0:
                ratio = (editorial_count/len(internal_links)*100)
                print(f"  üìä Ratio √©ditorial: {ratio:.1f}%")
            
            # Analyser les patterns
            analysis = self.analyze_linking_patterns(internal_links, website_url)
            
            # Ajouter analyse de qualit√© des liens √©ditoriaux
            quality_analysis = self.analyze_editorial_quality(internal_links, analysis['stats'])
            analysis.update(quality_analysis)
            
            # Analyser la qualit√© du contenu si les fichiers sont disponibles
            content_analysis = self.analyze_content_quality(csv_path, website_url, url_filter)
            if content_analysis:
                analysis['content_quality'] = content_analysis
            
            # Analyser les clusters s√©mantiques si disponibles
            if self.config.get('semantic_analysis', {}).get('enable_semantic_analysis', False):
                semantic_analysis = self.analyze_semantic_clusters(csv_path, website_url, url_filter)
                if semantic_analysis:
                    analysis['semantic_clusters'] = semantic_analysis
            
            # Analyse s√©mantique avanc√©e avec CamemBERT (automatique)
            if SEMANTIC_ANALYSIS_AVAILABLE:
                # Collecter les donn√©es de pages si disponibles
                page_data = {}
                if content_analysis and isinstance(content_analysis, dict):
                    # Essayer de collecter Title, H1, Meta descriptions depuis les analyses
                    page_data = self.collect_page_data_for_semantic(csv_path)
                
                # Filtrer pour ne garder que les liens √©ditoriaux
                editorial_links = [link for link in internal_links if not link.get('is_mechanical', False)]
                
                advanced_semantic = self.analyze_semantic_advanced(
                    editorial_links, 
                    column_mapping.get('anchor'), 
                    column_mapping.get('dest'),
                    page_data
                )
                if advanced_semantic:
                    analysis['advanced_semantic'] = advanced_semantic
            
            # G√©n√©rer les donn√©es pour le graphique de r√©seau
            network_data = self.generate_network_data(internal_links, column_mapping)
            analysis['network_data'] = network_data
            
            # G√©n√©rer le rapport
            report_file = self.generate_html_report(analysis, website_url, csv_path, url_filter)
            
            print(f"\n‚úÖ ANALYSE TERMIN√âE")
            print("="*50)
            print(f"üìÑ Rapport HTML: {report_file}")
            
            # Cr√©er un lien cliquable pour le terminal
            import os
            absolute_path = os.path.abspath(report_file)
            clickable_link = f"file://{absolute_path}"
            print(f"üîó Lien cliquable: {clickable_link}")
            
            print(f"üö´ Pages orphelines: {len(analysis['orphan_pages'])}")
            
            # Option pour ouvrir automatiquement
            try:
                import webbrowser
                print(f"\nüí° Ouverture automatique du rapport...")
                webbrowser.open(clickable_link)
                print(f"‚úÖ Rapport ouvert dans votre navigateur par d√©faut")
            except Exception as e:
                print(f"‚ö†Ô∏è  Impossible d'ouvrir automatiquement: {e}")
                print(f"üí° Ctrl+Click sur le lien ci-dessus pour l'ouvrir")
            
            return report_file
            
        except Exception as e:
            print(f"‚ùå Erreur critique lors de l'analyse: {e}")
            print("üí° V√©rifiez que le fichier CSV est valide et accessible")
            return None

    def identify_columns(self, fieldnames):
        """Identifie les colonnes importantes dans le CSV"""
        mapping = {
            'source': None,
            'dest': None,
            'anchor': None,
            'origin': None,
            'xpath': None
        }
        
        for field in fieldnames:
            field_lower = field.lower().strip()
            
            # Source
            if 'source' in field_lower and not mapping['source']:
                mapping['source'] = field
            
            # Destination
            elif any(term in field_lower for term in ['destination', 'target', 'dest']) and not mapping['dest']:
                mapping['dest'] = field
            
            # Ancre
            elif any(term in field_lower for term in ['anchor', 'ancrage', 'link text', 'text']) and not mapping['anchor']:
                mapping['anchor'] = field
            
            # Origine
            elif any(term in field_lower for term in ['origin', 'origine', 'type']) and not mapping['origin']:
                mapping['origin'] = field
            
            # XPath
            elif any(term in field_lower for term in ['xpath', 'chemin', 'path']) and not mapping['xpath']:
                mapping['xpath'] = field
        
        return mapping

    def analyze_linking_patterns(self, internal_links, website_url):
        """Analyse les patterns de maillage"""
        editorial_links = [link for link in internal_links if not link.get('is_mechanical', False)]
        
        # Trouver les colonnes
        source_col = dest_col = anchor_col = None
        if internal_links:
            for col in internal_links[0].keys():
                col_clean = col.lower().strip('\ufeff"')
                if 'source' in col_clean:
                    source_col = col
                elif 'destination' in col_clean:
                    dest_col = col
                elif 'anchor' in col_clean or 'ancrage' in col_clean:
                    anchor_col = col
        
        # Compteurs
        source_counter = Counter()
        dest_counter = Counter()
        anchor_counter = Counter()
        
        for link in editorial_links:
            if source_col:
                source_counter[link.get(source_col, '')] += 1
            if dest_col:
                dest_counter[link.get(dest_col, '')] += 1
            if anchor_col:
                anchor = link.get(anchor_col, '').strip()
                if anchor:
                    anchor_counter[anchor] += 1
        
        # Pages orphelines
        all_pages = set()
        linked_pages = set()
        
        for link in internal_links:
            if source_col:
                all_pages.add(link.get(source_col, ''))
            if dest_col and not link.get('is_mechanical', False):
                linked_pages.add(link.get(dest_col, ''))
        
        orphan_pages = all_pages - linked_pages
        
        return {
            'top_linking_pages': dict(source_counter.most_common(10)),
            'most_linked_pages': dict(dest_counter.most_common(10)),
            'top_anchors': dict(anchor_counter.most_common(20)),
            'over_optimized_anchors': {anchor: count for anchor, count in anchor_counter.items() if count > self.config.get('analysis_thresholds', {}).get('max_anchor_repetition', 5)},
            'orphan_pages': list(orphan_pages),
            'stats': {
                'total_pages': len(all_pages),
                'total_internal_links': len(internal_links),
                'editorial_links': len(editorial_links),
                'mechanical_links': len(internal_links) - len(editorial_links),
                'avg_editorial_per_page': len(editorial_links) / len(all_pages) if len(all_pages) > 0 else 0,
                'editorial_ratio': (len(editorial_links) / len(internal_links) * 100) if len(internal_links) > 0 else 0
            }
        }

    def analyze_editorial_quality(self, internal_links, stats):
        """Analyse la qualit√© des liens √©ditoriaux"""
        editorial_links = [link for link in internal_links if not link.get('is_mechanical', False)]
        
        # Trouver les colonnes
        anchor_col = dest_col = None
        if editorial_links:
            for col in editorial_links[0].keys():
                col_clean = col.lower().strip('\ufeff"')
                if 'anchor' in col_clean or 'ancrage' in col_clean:
                    anchor_col = col
                elif 'destination' in col_clean:
                    dest_col = col
        
        # Analyser la qualit√© des ancres
        anchor_quality = {
            'too_short': [],
            'too_long': [],
            'good_quality': [],
            'keyword_stuffed': [],
            'url_anchors': []
        }
        
        semantic_variations = {}  # Grouper les variations s√©mantiques
        
        for link in editorial_links:
            anchor = link.get(anchor_col, '').strip() if anchor_col else ''
            dest = link.get(dest_col, '') if dest_col else ''
            
            if not anchor:
                continue
                
            # Analyser la longueur
            word_count = len(anchor.split())
            if word_count == 1 and len(anchor) < 4:
                anchor_quality['too_short'].append({'anchor': anchor, 'dest': dest})
            elif word_count > 8:
                anchor_quality['too_long'].append({'anchor': anchor, 'dest': dest})
            elif 2 <= word_count <= 6:
                anchor_quality['good_quality'].append({'anchor': anchor, 'dest': dest})
            
            # D√©tecter le keyword stuffing (r√©p√©tition excessive de mots-cl√©s)
            words = anchor.lower().split()
            word_freq = {}
            for word in words:
                if len(word) > 3:  # Ignorer les mots courts
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            if any(freq > 2 for freq in word_freq.values()):
                anchor_quality['keyword_stuffed'].append({'anchor': anchor, 'dest': dest})
            
            # D√©tecter les ancres URL
            if any(anchor.startswith(prefix) for prefix in ['http://', 'https://', 'www.']):
                anchor_quality['url_anchors'].append({'anchor': anchor, 'dest': dest})
        
        # Analyse de la distribution th√©matique
        thematic_distribution = self.analyze_thematic_distribution(editorial_links, anchor_col, dest_col)
        
        return {
            'anchor_quality': anchor_quality,
            'thematic_distribution': thematic_distribution,
            'editorial_quality_score': self.calculate_editorial_score(
                anchor_quality, 
                len(editorial_links),
                stats['editorial_ratio'],
                stats['total_internal_links']
            )
        }
    
    def analyze_thematic_distribution(self, editorial_links, anchor_col, dest_col):
        """Analyse la distribution th√©matique des liens avec NLP avanc√©"""
        
        # Stop words fran√ßais √©tendus
        french_stop_words = {
            'le', 'de', 'et', '√†', 'un', 'il', '√™tre', 'et', 'en', 'avoir', 'que', 'pour',
            'dans', 'ce', 'son', 'une', 'sur', 'avec', 'ne', 'se', 'pas', 'tout', 'plus',
            'par', 'grand', 'en', 'une', '√™tre', 'et', '√†', 'il', 'avoir', 'ne', 'je', 'son',
            'que', 'se', 'qui', 'ce', 'dans', 'en', 'du', 'elle', 'au', 'de', 'le', 'un',
            'nous', 'vous', 'ils', 'elles', 'leur', 'leurs', 'cette', 'ces', 'ses', 'nos',
            'vos', 'tr√®s', 'bien', 'encore', 'toujours', 'd√©j√†', 'aussi', 'puis', 'donc',
            'ainsi', 'alors', 'apr√®s', 'avant', 'depuis', 'pendant', 'comme', 'quand',
            'comment', 'pourquoi', 'o√π', 'dont', 'laquelle', 'lequel', 'lesquels', 'desquels',
            'auquel', 'auxquels', 'duquel', 'desquelles', 'auxquelles', 'celle', 'celui',
            'ceux', 'celles', 'tout', 'tous', 'toute', 'toutes', 'autre', 'autres', 'm√™me',
            'm√™mes', 'tel', 'telle', 'tels', 'telles', 'quel', 'quelle', 'quels', 'quelles',
            'voir', 'savoir', 'faire', 'dire', 'aller', 'venir', 'pouvoir', 'vouloir',
            'devoir', 'falloir', 'prendre', 'donner', 'mettre', 'porter', 'tenir', 'venir',
            'partir', 'sortir', 'entrer', 'monter', 'descendre', 'passer', 'rester', 'devenir',
            'sembler', 'para√Ætre', 'appara√Ætre', 'dispara√Ætre', 'arriver', 'partir', 'na√Ætre',
            'mourir', 'vivre', 'exister', 'ici', 'l√†', 'ailleurs', 'partout', 'nulle', 'part',
            'quelque', 'part', 'jamais', 'toujours', 'souvent', 'parfois', 'quelquefois',
            'rarement', 'peu', 'beaucoup', 'trop', 'assez', 'tant', 'autant', 'si', 'aussi',
            'moins', 'davantage', 'plut√¥t', 'surtout', 'notamment', 'seulement', 'uniquement',
            'vraiment', 'certainement', 'probablement', 'peut', '√™tre', 'sans', 'doute',
            '√©videmment', 'naturellement', 'heureusement', 'malheureusement', 'd√©couvrir',
            'd√©couvrez', 'voir', 'lire', 'consulter', 'cliquer', 'acc√©der', 'suivre', 'plus',
            'notre', 'votre', 'leur', 'cette', 'cette', 'ces', 'tous', 'toutes'
        }
        
        # Mots g√©n√©riques suppl√©mentaires √† filtrer
        generic_words = {
            'page', 'site', 'web', 'internet', 'online', 'cliquez', 'ici', 'l√†', 'suivant',
            'pr√©c√©dent', 'retour', 'accueil', 'home', 'menu', 'navigation', 'lien', 'liens',
            'article', 'articles', 'actualit√©', 'actualit√©s', 'news', 'blog', 'post',
            'plus', 'moins', 'tout', 'tous', 'toute', 'toutes', 'autre', 'autres'
        }
        
        # Combine stop words
        all_stop_words = french_stop_words.union(generic_words)
        
        # Extraire et analyser les ancres
        anchor_texts = []
        dest_categories = Counter()
        
        for link in editorial_links:
            anchor = link.get(anchor_col, '').strip().lower() if anchor_col else ''
            dest = link.get(dest_col, '') if dest_col else ''
            
            if anchor and len(anchor) > 2:
                anchor_texts.append(anchor)
            
            # Cat√©goriser les destinations par type de page
            if dest:
                if any(term in dest.lower() for term in ['blog', 'article', 'actualit', 'news']):
                    dest_categories['Blog/Articles'] += 1
                elif any(term in dest.lower() for term in ['produit', 'product', 'service', 'solution']):
                    dest_categories['Produits/Services'] += 1
                elif any(term in dest.lower() for term in ['contact', 'about', 'propos', 'equipe', 'team']):
                    dest_categories['Pages institutionnelles'] += 1
                elif any(term in dest.lower() for term in ['expertise', 'competence', 'metier', 'domaine']):
                    dest_categories['Expertises'] += 1
                elif any(term in dest.lower() for term in ['carriere', 'emploi', 'job', 'recrutement']):
                    dest_categories['Carri√®res/Emploi'] += 1
                else:
                    dest_categories['Autres'] += 1
        
        # Analyse NLP avanc√©e des ancres
        semantic_keywords = self.extract_semantic_keywords(anchor_texts, all_stop_words)
        
        return {
            'top_anchor_keywords': semantic_keywords,
            'destination_categories': dict(dest_categories)
        }
    
    def extract_semantic_keywords(self, anchor_texts, stop_words):
        """Extraction de mots-cl√©s s√©mantiques avec techniques NLP"""
        
        # 1. Extraction des mots simples filtr√©s
        word_freq = Counter()
        bigram_freq = Counter()
        trigram_freq = Counter()
        
        for anchor in anchor_texts:
            # Nettoyer le texte
            clean_anchor = re.sub(r'[^\w\s]', ' ', anchor.lower())
            words = [w.strip() for w in clean_anchor.split() if len(w.strip()) >= 3]
            
            # Filtrer les stop words et mots g√©n√©riques
            meaningful_words = [w for w in words if w not in stop_words and len(w) >= 3]
            
            # Compter les mots simples
            for word in meaningful_words:
                if len(word) >= 4:  # Mots de 4 lettres minimum
                    word_freq[word] += 1
            
            # Compter les bigrammes (expressions de 2 mots)
            if len(meaningful_words) >= 2:
                for i in range(len(meaningful_words) - 1):
                    bigram = f"{meaningful_words[i]} {meaningful_words[i+1]}"
                    if len(bigram) >= 8:  # √âviter les bigrammes trop courts
                        bigram_freq[bigram] += 1
            
            # Compter les trigrammes (expressions de 3 mots)
            if len(meaningful_words) >= 3:
                for i in range(len(meaningful_words) - 2):
                    trigram = f"{meaningful_words[i]} {meaningful_words[i+1]} {meaningful_words[i+2]}"
                    if len(trigram) >= 12:  # √âviter les trigrammes trop courts
                        trigram_freq[trigram] += 1
        
        # 2. Calculer des scores de pertinence (simple TF-IDF-like)
        total_anchors = len(anchor_texts)
        scored_keywords = {}
        
        # Scorer les mots simples
        for word, freq in word_freq.items():
            if freq >= 2:  # Minimum 2 occurrences
                # Score bas√© sur fr√©quence et longueur du mot
                score = freq * (1 + len(word) / 10)
                scored_keywords[word] = {
                    'count': freq,
                    'score': score,
                    'type': 'mot'
                }
        
        # Scorer les bigrammes (bonus car plus informatifs)
        for bigram, freq in bigram_freq.items():
            if freq >= 2:  # Minimum 2 occurrences
                score = freq * 2.5  # Bonus pour les expressions
                scored_keywords[bigram] = {
                    'count': freq,
                    'score': score,
                    'type': 'expression'
                }
        
        # Scorer les trigrammes (bonus encore plus √©lev√©)
        for trigram, freq in trigram_freq.items():
            if freq >= 2:  # Minimum 2 occurrences
                score = freq * 4  # Gros bonus pour les expressions longues
                scored_keywords[trigram] = {
                    'count': freq,
                    'score': score,
                    'type': 'expression longue'
                }
        
        # 3. Trier par score et retourner le top 15
        sorted_keywords = sorted(scored_keywords.items(), key=lambda x: x[1]['score'], reverse=True)
        
        # Formater pour compatibilit√© avec l'ancien format
        result = {}
        for keyword, data in sorted_keywords[:15]:
            result[keyword] = data['count']
        
        return result
    
    def analyze_semantic_advanced(self, editorial_links, anchor_col, dest_col, page_data=None):
        """Analyse s√©mantique avanc√©e avec CamemBERT"""
        if not SEMANTIC_ANALYSIS_AVAILABLE:
            print("‚ö†Ô∏è  Analyse s√©mantique avanc√©e non disponible (d√©pendances manquantes)")
            return None
        
        print("\nüß† ANALYSE S√âMANTIQUE AVANC√âE (CamemBERT)")
        print("=" * 50)
        
        # Obtenir l'analyseur s√©mantique
        analyzer = get_semantic_analyzer()
        
        # Afficher les stats du cache
        cache_stats = analyzer.get_cache_stats()
        print(f"üì¶ Cache: {cache_stats['files']} entr√©es ({cache_stats['size_mb']} MB)")
        
        results = {}
        
        # 1. Clustering s√©mantique des ancres
        anchors = []
        for link in editorial_links:
            anchor = link.get(anchor_col, '').strip() if anchor_col else ''
            if anchor and len(anchor) > 3:
                anchors.append(anchor)
        
        if anchors:
            semantic_clusters = analyzer.cluster_semantic_themes(anchors, min_cluster_size=2)
            results['semantic_clusters'] = semantic_clusters
            
            if semantic_clusters:
                print(f"üéØ {len(semantic_clusters)} th√®mes s√©mantiques identifi√©s:")
                for theme, anchor_list in semantic_clusters.items():
                    print(f"   ‚Ä¢ {theme} ({len(anchor_list)} ancres)")
            else:
                print("‚ö†Ô∏è  Aucun cluster s√©mantique significatif trouv√©")
        
        # 2. Analyse de coh√©rence ancre ‚Üî contenu (si donn√©es disponibles)
        if page_data:
            print("üîç Analyse de coh√©rence ancres ‚Üî contenus de pages...")
            
            anchor_texts = []
            page_contents = []
            
            for link in editorial_links[:50]:  # Limiter pour les performances
                anchor = link.get(anchor_col, '').strip() if anchor_col else ''
                dest_url = link.get(dest_col, '') if dest_col else ''
                
                if anchor and dest_url and dest_url in page_data:
                    anchor_texts.append(anchor)
                    # Combiner toutes les donn√©es textuelles disponibles
                    content_parts = []
                    page_info = page_data[dest_url]
                    
                    # Priorit√© aux √©l√©ments les plus importants
                    priority_order = ['titles', 'h1', 'h2', 'h3', 'meta_desc', 'meta_keywords']
                    for data_type in priority_order:
                        if data_type in page_info and page_info[data_type]:
                            content_parts.append(page_info[data_type])
                    
                    # Ajouter images alt text si disponible (moins prioritaire)
                    if 'images_alt' in page_info and page_info['images_alt']:
                        # Limiter le alt text pour √©viter la pollution
                        alt_text = page_info['images_alt'][:200]  # Max 200 chars
                        content_parts.append(alt_text)
                    
                    combined_content = ' '.join(content_parts)
                    page_contents.append(combined_content)
            
            if anchor_texts and page_contents:
                coherence_scores = analyzer.analyze_semantic_coherence(anchor_texts, page_contents)
                avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 0
                
                results['coherence_analysis'] = {
                    'average_score': avg_coherence,
                    'total_analyzed': len(coherence_scores),
                    'high_coherence': len([s for s in coherence_scores if s > 0.7]),
                    'low_coherence': len([s for s in coherence_scores if s < 0.4])
                }
                
                print(f"   üìä Score moyen de coh√©rence: {avg_coherence:.2f}")
                print(f"   ‚úÖ Liens tr√®s coh√©rents (>0.7): {results['coherence_analysis']['high_coherence']}")
                print(f"   ‚ö†Ô∏è  Liens peu coh√©rents (<0.4): {results['coherence_analysis']['low_coherence']}")
        
        # 3. Recherche d'opportunit√©s de maillage
        if page_data and len(page_data) > 1:
            print("üîç Recherche d'opportunit√©s de maillage...")
            
            # Pr√©parer les contenus enrichis des pages
            page_contents_for_gaps = {}
            for url, data in list(page_data.items())[:100]:  # Limiter pour les performances
                content_parts = []
                
                # Utiliser toutes les donn√©es disponibles pour une meilleure similarit√©
                priority_order = ['titles', 'h1', 'h2', 'h3', 'meta_desc']
                for data_type in priority_order:
                    if data_type in data and data[data_type]:
                        content_parts.append(data[data_type])
                
                # Ajouter meta keywords si disponibles (utiles pour la similarit√©)
                if 'meta_keywords' in data and data['meta_keywords']:
                    content_parts.append(data['meta_keywords'])
                
                if content_parts:
                    combined_content = ' '.join(content_parts)
                    page_contents_for_gaps[url] = combined_content
            
            if len(page_contents_for_gaps) > 1:
                opportunities = analyzer.find_semantic_gaps(page_contents_for_gaps, threshold=0.6)
                results['link_opportunities'] = opportunities
                
                if opportunities:
                    print(f"   üöÄ {len(opportunities)} opportunit√©s de maillage trouv√©es")
                    for i, (url1, url2, similarity) in enumerate(opportunities[:5]):
                        print(f"   {i+1}. Similarit√© {similarity:.2f}: {url1} ‚Üî {url2}")
                else:
                    print("   ‚ÑπÔ∏è  Aucune opportunit√© significative trouv√©e")
        
        return results
    
    def collect_page_data_for_semantic(self, csv_base_path):
        """Collecter les donn√©es textuelles des pages pour l'analyse s√©mantique"""
        page_data = {}
        
        # Obtenir le dossier de base des fichiers CSV
        base_dir = os.path.dirname(csv_base_path)
        base_name = os.path.splitext(os.path.basename(csv_base_path))[0]
        
        # Patterns de fichiers √† chercher (enrichis)
        file_patterns = {
            'titles': ['page_titles', 'titles', 'titres'],
            'h1': ['h1', 'h1_1'],
            'h2': ['h2', 'h2_1'],
            'h3': ['h3', 'h3_1'],
            'meta_desc': ['meta_description', 'description', 'meta_descriptions'],
            'meta_keywords': ['meta_keywords', 'keywords', 'mots_cles'],
            'images_alt': ['images', 'image_alt', 'alt_text'],
            'schema_org': ['schema', 'structured_data', 'schema_org']
        }
        
        # Chercher et charger chaque type de fichier
        for data_type, patterns in file_patterns.items():
            found_file = None
            
            for pattern in patterns:
                # Essayer diff√©rents formats de noms
                for filename in [
                    f"{pattern}.csv",
                    f"{base_name}_{pattern}.csv",
                    f"all_{pattern}.csv",
                    f"tous_les_{pattern}.csv"
                ]:
                    filepath = os.path.join(base_dir, filename)
                    if os.path.exists(filepath):
                        found_file = filepath
                        break
                
                if found_file:
                    break
            
            # Charger le fichier trouv√©
            if found_file:
                try:
                    rows, fieldnames = self.load_csv_file(found_file)
                    if rows:
                        print(f"‚úÖ Trouv√© pour analyse s√©mantique: {data_type} -> {os.path.basename(found_file)}")
                        
                        # Identifier les colonnes URL et contenu
                        url_col = None
                        content_col = None
                        
                        for col in fieldnames:
                            col_lower = col.lower()
                            if 'address' in col_lower or 'url' in col_lower:
                                url_col = col
                            elif self._matches_content_column(data_type, col_lower):
                                content_col = col
                        
                        # Extraire les donn√©es
                        if url_col and content_col:
                            for row in rows:
                                url = row.get(url_col, '').strip()
                                content = row.get(content_col, '').strip()
                                
                                if url and content:
                                    if url not in page_data:
                                        page_data[url] = {}
                                    page_data[url][data_type] = content
                
                except Exception as e:
                    print(f"‚ö†Ô∏è  Erreur lors du chargement de {found_file}: {e}")
        
        if page_data:
            print(f"üìä Donn√©es collect√©es pour {len(page_data)} pages pour l'analyse s√©mantique")
        
        return page_data
    
    def _matches_content_column(self, data_type, col_lower):
        """D√©terminer si une colonne correspond au type de contenu recherch√©"""
        column_patterns = {
            'titles': ['title', 'titre', 'page title'],
            'h1': ['h1', 'heading 1', 'titre 1'],
            'h2': ['h2', 'heading 2', 'titre 2'],
            'h3': ['h3', 'heading 3', 'titre 3'],
            'meta_desc': ['description', 'meta description', 'meta_description'],
            'meta_keywords': ['keywords', 'meta keywords', 'meta_keywords', 'mots', 'cles'],
            'images_alt': ['alt', 'alt text', 'alternative', 'image alt'],
            'schema_org': ['schema', 'structured', 'microdata', 'json-ld']
        }
        
        if data_type in column_patterns:
            return any(pattern in col_lower for pattern in column_patterns[data_type])
        
        return False
    
    def generate_semantic_analysis_section(self, semantic_data):
        """G√©n√©rer la section d'analyse s√©mantique avec graphiques"""
        
        html_content = """
        <div class="section">
            <h2>Analyse s√©mantique avanc√©e (CamemBERT)</h2>
        """
        
        # 1. Clustering s√©mantique avec graphiques
        if 'semantic_clusters' in semantic_data and semantic_data['semantic_clusters']:
            clusters = semantic_data['semantic_clusters']
            total_anchors = sum(len(anchors) for anchors in clusters.values())
            
            # V√©rifier si l'analyse est pertinente (plus d'un cluster ou diversit√© suffisante)
            if len(clusters) == 1:
                cluster_name = list(clusters.keys())[0]
                cluster_anchors = list(clusters.values())[0]
                unique_anchors = set(anchor.lower().strip() for anchor in cluster_anchors)
                diversity_ratio = len(unique_anchors) / len(cluster_anchors)
                
                if diversity_ratio < 0.3:  # Faible diversit√©
                    html_content += f"""
                    <div class="warning">
                        <h3>‚ö†Ô∏è Analyse s√©mantique non pertinente</h3>
                        <p><strong>Probl√®me d√©tect√© :</strong> Faible diversit√© des ancres de liens</p>
                        <ul>
                            <li>üìä Ancres analys√©es : {total_anchors}</li>
                            <li>üîÑ Ancres uniques : {len(unique_anchors)} ({diversity_ratio:.1%})</li>
                            <li>üéØ Cluster unique : "{cluster_name}"</li>
                        </ul>
                        
                        <h4>üí° Recommandations pour am√©liorer l'analyse s√©mantique :</h4>
                        <ol>
                            <li><strong>Diversifier les ancres :</strong> Utiliser des termes vari√©s et descriptifs</li>
                            <li><strong>√âviter la r√©p√©tition :</strong> "{cluster_anchors[0] if cluster_anchors else 'N/A'}" appara√Æt {cluster_anchors.count(cluster_anchors[0]) if cluster_anchors else 0} fois</li>
                            <li><strong>Ancres contextuelles :</strong> D√©crire le contenu de destination plut√¥t que le nom du site</li>
                            <li><strong>Synonymes et variations :</strong> "services", "expertise", "solutions", "conseil"...</li>
                            <li><strong>Ancres longue tra√Æne :</strong> "extension bois Amiens", "terrasse composite", "permis de construire"</li>
                        </ol>
                        
                        <div class="success">
                            <p><strong>‚úÖ Objectif :</strong> Atteindre au moins 30% de diversit√© pour une analyse s√©mantique utile</p>
                        </div>
                    </div>
                    """
                    html_content += "</div>"  # Fermer la section
                    return html_content
            
            # Si on arrive ici, l'analyse est pertinente (diversit√© suffisante ou plusieurs clusters)
            html_content += f"""
            <div class="semantic-analysis">
                <h3>Th√®mes s√©mantiques identifi√©s ({len(clusters)} clusters, {total_anchors} ancres)</h3>
                
                <div class="chart-container">
                    <div class="chart">
                        <h4>R√©partition des th√®mes</h4>
                        <div class="pie-chart-semantic">
            """
            
            # Graphique en secteurs des th√®mes
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FF8A80', '#FFD93D', '#6C5CE7', '#FD79A8']
            for i, (theme, anchors) in enumerate(clusters.items()):
                percentage = (len(anchors) / total_anchors) * 100
                color = colors[i % len(colors)]
                html_content += f"""
                            <div class="pie-item-semantic">
                                <span class="pie-color" style="background-color: {color}"></span>
                                <span class="pie-label-semantic">{theme}: {len(anchors)} ancres ({percentage:.1f}%)</span>
                            </div>
                """
            
            html_content += """
                        </div>
                    </div>
                    
                    <div class="chart">
                        <h4>D√©tail par th√®me</h4>
                        <div class="themes-detail">
            """
            
            # Graphique en barres horizontales avec d√©tails
            max_anchors = max(len(anchors) for anchors in clusters.values())
            for i, (theme, anchors) in enumerate(clusters.items()):
                percentage = (len(anchors) / max_anchors) * 100
                color = colors[i % len(colors)]
                
                html_content += f"""
                            <div class="theme-item">
                                <div class="theme-header">
                                    <span class="theme-name">{theme}</span>
                                    <span class="theme-count">{len(anchors)} ancres</span>
                                </div>
                                <div class="theme-bar-container">
                                    <div class="theme-bar" style="width: {percentage}%; background-color: {color}"></div>
                                </div>
                                <div class="theme-examples">
                """
                
                # Afficher quelques exemples d'ancres
                example_anchors = anchors[:5]  # Top 5 exemples
                for anchor in example_anchors:
                    html_content += f'<span class="anchor-example">"{anchor}"</span>'
                
                if len(anchors) > 5:
                    html_content += f'<span class="anchor-more">... et {len(anchors) - 5} autres</span>'
                
                html_content += """
                                </div>
                            </div>
                """
            
            html_content += """
                        </div>
                    </div>
                </div>
            """
            
            # 3. Nuage de mots par th√®me
            html_content += """
                <h4>Nuages de mots par th√®me</h4>
                <div class="word-clouds-container">
            """
            
            for i, (theme, anchors) in enumerate(clusters.items()):
                color = colors[i % len(colors)]
                
                # Extraire les mots les plus fr√©quents du th√®me
                all_words = []
                for anchor in anchors:
                    words = anchor.lower().split()
                    all_words.extend([w for w in words if len(w) > 3])
                
                from collections import Counter
                word_freq = Counter(all_words)
                top_words = word_freq.most_common(10)
                
                html_content += f"""
                    <div class="word-cloud-theme" style="border-left: 4px solid {color}">
                        <h5>{theme}</h5>
                        <div class="word-cloud-mini">
                """
                
                for word, freq in top_words:
                    # Taille bas√©e sur la fr√©quence (min 0.8em, max 1.6em)
                    size = 0.8 + (freq / max(1, max(f for _, f in top_words))) * 0.8
                    html_content += f'<span class="word-mini" style="font-size: {size}em; color: {color}">{word}</span>'
                
                html_content += """
                        </div>
                    </div>
                """
            
            html_content += "</div>"
        
        # 2. Analyse de coh√©rence si disponible
        if 'coherence_analysis' in semantic_data:
            coherence = semantic_data['coherence_analysis']
            html_content += f"""
            <div class="coherence-analysis">
                <h3>Coh√©rence s√©mantique ancres ‚Üî contenus</h3>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{coherence['average_score']:.2f}</div>
                        <div class="stat-label">Score moyen</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{coherence['high_coherence']}</div>
                        <div class="stat-label">Liens tr√®s coh√©rents (>0.7)</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{coherence['low_coherence']}</div>
                        <div class="stat-label">Liens peu coh√©rents (<0.4)</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{coherence['total_analyzed']}</div>
                        <div class="stat-label">Liens analys√©s</div>
                    </div>
                </div>
            </div>
            """
        
        # 3. Opportunit√©s de maillage si disponibles
        if 'link_opportunities' in semantic_data and semantic_data['link_opportunities']:
            opportunities = semantic_data['link_opportunities'][:10]  # Top 10
            html_content += f"""
            <div class="opportunities-analysis">
                <h3>Opportunit√©s de maillage d√©tect√©es</h3>
                <p>Pages s√©mantiquement similaires qui pourraient √™tre li√©es :</p>
                <table>
                    <tr><th>Similarit√©</th><th>Page 1</th><th>Page 2</th></tr>
            """
            
            for url1, url2, similarity in opportunities:
                similarity_percent = similarity * 100
                color = '#28a745' if similarity > 0.8 else '#ffc107' if similarity > 0.6 else '#dc3545'
                html_content += f"""
                    <tr>
                        <td><span style="color: {color}; font-weight: bold">{similarity_percent:.1f}%</span></td>
                        <td class="url">{url1}</td>
                        <td class="url">{url2}</td>
                    </tr>
                """
            
            html_content += "</table></div>"
        
        # Si aucun clustering n'est disponible
        else:
            html_content += """
            <div class="warning">
                <h3>‚ÑπÔ∏è Analyse s√©mantique non disponible</h3>
                <p><strong>Raisons possibles :</strong></p>
                <ul>
                    <li>üî¢ Trop peu d'ancres de liens (minimum 3 requis)</li>
                    <li>üîÑ Diversit√© insuffisante des ancres (&lt; 30%)</li>
                    <li>‚öôÔ∏è Erreur lors du traitement NLP</li>
                </ul>
                
                <h4>üí° Pour activer l'analyse s√©mantique :</h4>
                <ol>
                    <li><strong>Augmenter le nombre de liens √©ditoriaux</strong> (minimum 10 recommand√©)</li>
                    <li><strong>Diversifier les ancres de liens :</strong>
                        <ul>
                            <li>"nos services de construction"</li>
                            <li>"expertise en extension bois"</li>
                            <li>"solutions d'am√©nagement"</li>
                            <li>"conseil en r√©novation"</li>
                        </ul>
                    </li>
                    <li><strong>√âviter les ancres g√©n√©riques :</strong> "cliquez ici", "en savoir plus"</li>
                    <li><strong>Utiliser des termes m√©tiers sp√©cifiques</strong> √† votre domaine</li>
                </ol>
            </div>
            """
        
        html_content += "</div>"  # Fermer la section
        
        return html_content
    
    def calculate_editorial_score(self, anchor_quality, total_editorial, editorial_ratio, total_internal_links):
        """Calcule un score de qualit√© √©ditorial (0-100)"""
        if total_editorial == 0:
            return 0
        
        # Score de base en fonction du ratio √©ditorial (poids le plus important)
        # Ratio id√©al : 70-80%, acceptable : 50-70%, faible < 50%
        if editorial_ratio >= 70:
            base_score = 90
        elif editorial_ratio >= 50:
            base_score = 70
        elif editorial_ratio >= 30:
            base_score = 50
        elif editorial_ratio >= 15:
            base_score = 30
        else:
            base_score = 10  # Tr√®s faible ratio √©ditorial
        
        # P√©nalit√©s sur la qualit√© des ancres (proportionnelles au score de base)
        penalty_factor = base_score / 100  # R√©duire les p√©nalit√©s si le score de base est d√©j√† bas
        
        too_short_penalty = (len(anchor_quality['too_short']) / total_editorial) * 15 * penalty_factor
        too_long_penalty = (len(anchor_quality['too_long']) / total_editorial) * 10 * penalty_factor
        keyword_stuffed_penalty = (len(anchor_quality['keyword_stuffed']) / total_editorial) * 20 * penalty_factor
        url_anchors_penalty = (len(anchor_quality['url_anchors']) / total_editorial) * 25 * penalty_factor
        
        # Bonus limit√© pour les bonnes ancres (max +5 points)
        good_quality_ratio = len(anchor_quality['good_quality']) / total_editorial
        good_quality_bonus = min(5, good_quality_ratio * 10)
        
        # P√©nalit√© suppl√©mentaire si tr√®s peu de liens √©ditoriaux au total
        if total_editorial < 50:
            volume_penalty = 10
        elif total_editorial < 20:
            volume_penalty = 20
        else:
            volume_penalty = 0
        
        final_score = base_score - too_short_penalty - too_long_penalty - keyword_stuffed_penalty - url_anchors_penalty + good_quality_bonus - volume_penalty
        
        # S'assurer que le score reste entre 0 et 100
        final_score = max(0, min(100, final_score))
        
        return round(final_score, 1)

    def analyze_content_quality(self, csv_path, website_url, url_filter=None):
        """Analyse la qualit√© du contenu en cherchant les fichiers CSV suppl√©mentaires"""
        base_path = os.path.dirname(csv_path)
        content_data = {}
        
        # Fichiers CSV √† analyser
        csv_files = {
            'word_count': ['word_count', 'nombre_mots', 'wordcount'],
            'page_titles': ['page_titles', 'titles', 'titres'],
            'h1': ['h1', 'h1_1'],
            'internal': ['internal', 'pages_internes', 'all_pages']
        }
        
        print(f"\nüîç ANALYSE DE QUALIT√â DU CONTENU")
        print("="*50)
        
        # Chercher les fichiers dans le dossier
        available_files = {}
        if os.path.exists(base_path):
            for file in os.listdir(base_path):
                if file.endswith('.csv'):
                    file_lower = file.lower()
                    for data_type, patterns in csv_files.items():
                        if any(pattern in file_lower for pattern in patterns):
                            available_files[data_type] = os.path.join(base_path, file)
                            print(f"‚úÖ Trouv√©: {data_type} -> {file}")
        
        if not available_files:
            print("‚ö†Ô∏è  Aucun fichier de contenu trouv√© - utilisez l'export complet de Screaming Frog")
            return None
        
        # Charger et analyser les donn√©es
        analysis_results = {}
        
        # 1. Analyse du nombre de mots
        if 'word_count' in available_files:
            word_analysis = self.analyze_word_count(available_files['word_count'], url_filter)
            if word_analysis:
                analysis_results['word_analysis'] = word_analysis
        
        # 2. Analyse des titres et H1
        if 'page_titles' in available_files and 'h1' in available_files:
            title_h1_analysis = self.analyze_title_h1_coherence(
                available_files['page_titles'], 
                available_files['h1'], 
                url_filter
            )
            if title_h1_analysis:
                analysis_results['title_h1_coherence'] = title_h1_analysis
        
        # 3. Identification des pages de conversion
        if 'internal' in available_files:
            conversion_analysis = self.identify_conversion_pages(available_files['internal'], url_filter)
            if conversion_analysis:
                analysis_results['conversion_pages'] = conversion_analysis
        
        return analysis_results if analysis_results else None

    def analyze_word_count(self, csv_path, url_filter=None):
        """Analyse le nombre de mots par page"""
        try:
            rows, fieldnames = self.load_csv_file(csv_path)
            if not rows:
                return None
            
            print(f"üìä Analyse du nombre de mots ({len(rows):,} pages)")
            
            # Filtrer si n√©cessaire
            if url_filter:
                original_count = len(rows)
                rows = [row for row in rows if any(
                    str(row.get(col, '')).startswith(url_filter) 
                    for col in row.keys() if 'url' in col.lower() or 'address' in col.lower()
                )]
                print(f"üéØ Apr√®s filtrage: {len(rows):,} pages ({original_count - len(rows):,} supprim√©es)")
            
            # Identifier la colonne de mots
            word_col = None
            url_col = None
            for col in fieldnames:
                col_lower = col.lower()
                if any(term in col_lower for term in ['word', 'mots', 'count']):
                    word_col = col
                elif any(term in col_lower for term in ['address', 'url', 'source']):
                    url_col = col
            
            if not word_col or not url_col:
                print("‚ùå Colonnes 'word count' ou 'URL' non trouv√©es")
                return None
            
            # Analyser les donn√©es
            word_counts = []
            for row in rows:
                try:
                    word_count = int(row.get(word_col, 0))
                    url = row.get(url_col, '')
                    if word_count >= 0 and url:  # Exclure les valeurs n√©gatives
                        word_counts.append({'url': url, 'word_count': word_count})
                except (ValueError, TypeError):
                    continue
            
            if not word_counts:
                return None
            
            # Statistiques
            counts = [item['word_count'] for item in word_counts]
            
            analysis = {
                'total_pages': len(word_counts),
                'avg_words': sum(counts) / len(counts),
                'median_words': sorted(counts)[len(counts)//2],
                'min_words': min(counts),
                'max_words': max(counts),
                'thin_content': [item for item in word_counts if item['word_count'] < 300],
                'rich_content': [item for item in word_counts if item['word_count'] > 1500],
                'quality_content': [item for item in word_counts if 300 <= item['word_count'] <= 1500]
            }
            
            print(f"üìà Statistiques:")
            print(f"  - Moyenne: {analysis['avg_words']:.0f} mots")
            print(f"  - Contenu thin (< 300): {len(analysis['thin_content'])} pages")
            print(f"  - Contenu riche (> 1500): {len(analysis['rich_content'])} pages")
            print(f"  - Contenu qualit√© (300-1500): {len(analysis['quality_content'])} pages")
            
            return analysis
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse des mots: {e}")
            return None

    def analyze_title_h1_coherence(self, titles_csv, h1_csv, url_filter=None):
        """Analyse la coh√©rence entre les titres et H1"""
        try:
            # Charger les deux fichiers
            titles_data, _ = self.load_csv_file(titles_csv)
            h1_data, _ = self.load_csv_file(h1_csv)
            
            if not titles_data or not h1_data:
                return None
            
            print(f"üè∑Ô∏è  Analyse coh√©rence Title/H1")
            
            # Cr√©er des dictionnaires par URL
            titles_dict = {}
            h1_dict = {}
            
            # Parser les titres
            for row in titles_data:
                url = None
                title = None
                for col, value in row.items():
                    col_lower = col.lower()
                    if any(term in col_lower for term in ['address', 'url', 'source']):
                        url = value
                    elif any(term in col_lower for term in ['title', 'titre']):
                        title = value
                
                if url and title and (not url_filter or url.startswith(url_filter)):
                    titles_dict[url] = title
            
            # Parser les H1
            for row in h1_data:
                url = None
                h1 = None
                for col, value in row.items():
                    col_lower = col.lower()
                    if any(term in col_lower for term in ['address', 'url', 'source']):
                        url = value
                    elif 'h1' in col_lower:
                        h1 = value
                
                if url and h1 and (not url_filter or url.startswith(url_filter)):
                    h1_dict[url] = h1
            
            # Analyser la coh√©rence
            coherence_analysis = {
                'total_pages_with_both': 0,
                'identical': [],
                'similar': [],
                'different': [],
                'missing_h1': [],
                'missing_title': []
            }
            
            all_urls = set(titles_dict.keys()) | set(h1_dict.keys())
            
            for url in all_urls:
                title = titles_dict.get(url, '')
                h1 = h1_dict.get(url, '')
                
                if not title:
                    coherence_analysis['missing_title'].append(url)
                elif not h1:
                    coherence_analysis['missing_h1'].append(url)
                else:
                    coherence_analysis['total_pages_with_both'] += 1
                    
                    # Comparaison
                    if title.strip().lower() == h1.strip().lower():
                        coherence_analysis['identical'].append({'url': url, 'text': title})
                    elif self.similarity_score(title, h1) > 0.7:
                        coherence_analysis['similar'].append({
                            'url': url, 'title': title, 'h1': h1
                        })
                    else:
                        coherence_analysis['different'].append({
                            'url': url, 'title': title, 'h1': h1
                        })
            
            print(f"üìä R√©sultats coh√©rence:")
            print(f"  - Pages avec Title et H1: {coherence_analysis['total_pages_with_both']}")
            print(f"  - Identiques: {len(coherence_analysis['identical'])}")
            print(f"  - Similaires: {len(coherence_analysis['similar'])}")
            print(f"  - Diff√©rents: {len(coherence_analysis['different'])}")
            
            return coherence_analysis
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse Title/H1: {e}")
            return None

    def identify_conversion_pages(self, csv_path, url_filter=None):
        """Identifie les pages de conversion potentielles"""
        try:
            rows, fieldnames = self.load_csv_file(csv_path)
            if not rows:
                return None
            
            print(f"üí∞ Identification des pages de conversion")
            
            # Patterns de conversion
            conversion_patterns = {
                'contact': ['contact', 'nous-contacter', 'contactez', 'get-in-touch'],
                'achat': ['achat', 'acheter', 'buy', 'purchase', 'commande', 'commander', 'order'],
                'inscription': ['inscription', 'register', 'signup', 'sign-up', 's-inscrire'],
                'devis': ['devis', 'quote', 'estimation', 'demande', 'request'],
                'panier': ['panier', 'cart', 'basket', 'checkout'],
                'pricing': ['prix', 'pricing', 'tarifs', 'rates', 'cost'],
                'demo': ['demo', 'demonstration', 'essai', 'trial', 'test']
            }
            
            conversion_pages = {category: [] for category in conversion_patterns.keys()}
            
            # Identifier la colonne URL
            url_col = None
            title_col = None
            for col in fieldnames:
                col_lower = col.lower()
                if any(term in col_lower for term in ['address', 'url', 'source']):
                    url_col = col
                elif any(term in col_lower for term in ['title', 'titre']):
                    title_col = col
            
            if not url_col:
                print("‚ùå Colonne URL non trouv√©e")
                return None
            
            for row in rows:
                url = row.get(url_col, '').lower()
                title = row.get(title_col, '').lower() if title_col else ''
                
                # Appliquer le filtre
                if url_filter and not row.get(url_col, '').startswith(url_filter):
                    continue
                
                # V√©rifier les patterns
                full_text = f"{url} {title}"
                for category, patterns in conversion_patterns.items():
                    if any(pattern in full_text for pattern in patterns):
                        conversion_pages[category].append({
                            'url': row.get(url_col, ''),
                            'title': row.get(title_col, '') if title_col else '',
                            'category': category
                        })
                        break  # Une page ne peut √™tre que dans une cat√©gorie
            
            # Statistiques
            total_conversion_pages = sum(len(pages) for pages in conversion_pages.values())
            print(f"üìä Pages de conversion trouv√©es: {total_conversion_pages}")
            
            for category, pages in conversion_pages.items():
                if pages:
                    print(f"  - {category.title()}: {len(pages)} pages")
            
            return {
                'by_category': conversion_pages,
                'total_conversion_pages': total_conversion_pages,
                'conversion_rate_potential': total_conversion_pages / len(rows) * 100 if rows else 0
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'identification des pages de conversion: {e}")
            return None

    def similarity_score(self, text1, text2):
        """Calcule un score de similarit√© simple entre deux textes"""
        if not text1 or not text2:
            return 0
        
        # Normaliser
        t1 = set(text1.lower().split())
        t2 = set(text2.lower().split())
        
        # Jaccard similarity
        intersection = len(t1 & t2)
        union = len(t1 | t2)
        
        return intersection / union if union > 0 else 0

    def analyze_semantic_clusters(self, csv_path, website_url, url_filter=None):
        """Analyse les clusters s√©mantiques g√©n√©r√©s par Screaming Frog v22+"""
        base_path = os.path.dirname(csv_path)
        
        # Fichiers s√©mantiques √† rechercher
        semantic_files = {
            'embeddings': ['embeddings', 'embedding'],
            'similar_pages': ['similar', 'cluster', 'semantic'],
            'content_clusters': ['content_cluster', 'clusters']
        }
        
        print(f"\nüß† ANALYSE S√âMANTIQUE (Screaming Frog v22+)")
        print("="*50)
        
        # Chercher les fichiers s√©mantiques
        available_files = {}
        if os.path.exists(base_path):
            for file in os.listdir(base_path):
                if file.endswith('.csv'):
                    file_lower = file.lower()
                    for data_type, patterns in semantic_files.items():
                        if any(pattern in file_lower for pattern in patterns):
                            available_files[data_type] = os.path.join(base_path, file)
                            print(f"‚úÖ Trouv√©: {data_type} -> {file}")
        
        if not available_files:
            print("‚ö†Ô∏è  Aucun fichier s√©mantique trouv√©")
            print("üí° Activez l'analyse s√©mantique dans Screaming Frog (Config > API Access > AI)")
            return None
        
        analysis_results = {}
        
        # 1. Analyse des pages similaires
        if 'similar_pages' in available_files:
            similarity_analysis = self.analyze_similar_pages(available_files['similar_pages'], url_filter)
            if similarity_analysis:
                analysis_results['similar_pages'] = similarity_analysis
        
        # 2. Analyse des clusters de contenu
        if 'content_clusters' in available_files:
            cluster_analysis = self.analyze_content_clusters(available_files['content_clusters'], url_filter)
            if cluster_analysis:
                analysis_results['content_clusters'] = cluster_analysis
        
        # 3. Recommandations de maillage s√©mantique
        if analysis_results:
            linking_recommendations = self.generate_semantic_linking_recommendations(analysis_results)
            analysis_results['linking_recommendations'] = linking_recommendations
        
        return analysis_results if analysis_results else None

    def analyze_similar_pages(self, csv_path, url_filter=None):
        """Analyse les pages s√©mantiquement similaires"""
        try:
            rows, fieldnames = self.load_csv_file(csv_path)
            if not rows:
                return None
                
            print(f"üîç Analyse de similarit√© s√©mantique ({len(rows):,} relations)")
            
            # Filtrer si n√©cessaire
            if url_filter:
                original_count = len(rows)
                rows = [row for row in rows if any(
                    str(row.get(col, '')).startswith(url_filter) 
                    for col in row.keys() if 'url' in col.lower() or 'source' in col.lower()
                )]
                print(f"üéØ Apr√®s filtrage: {len(rows):,} relations ({original_count - len(rows):,} supprim√©es)")
            
            # Identifier les colonnes
            source_col = target_col = similarity_col = None
            for col in fieldnames:
                col_lower = col.lower()
                if any(term in col_lower for term in ['source', 'page', 'url']) and not target_col:
                    source_col = col
                elif any(term in col_lower for term in ['target', 'similar', 'related']) and source_col:
                    target_col = col
                elif any(term in col_lower for term in ['similarity', 'score', 'distance']):
                    similarity_col = col
            
            if not source_col or not target_col:
                print("‚ùå Colonnes source/target non trouv√©es")
                return None
            
            # Analyser les relations
            similarity_threshold = self.config.get('semantic_analysis', {}).get('similarity_threshold', 0.85)
            high_similarity_pairs = []
            similarity_scores = []
            
            for row in rows:
                try:
                    source = row.get(source_col, '')
                    target = row.get(target_col, '')
                    score = float(row.get(similarity_col, 0)) if similarity_col else 1.0
                    
                    if source and target and score >= similarity_threshold:
                        high_similarity_pairs.append({
                            'source': source,
                            'target': target,
                            'similarity': score
                        })
                        similarity_scores.append(score)
                        
                except (ValueError, TypeError):
                    continue
            
            if not high_similarity_pairs:
                return None
            
            analysis = {
                'total_similar_pairs': len(high_similarity_pairs),
                'avg_similarity': sum(similarity_scores) / len(similarity_scores),
                'high_similarity_threshold': similarity_threshold,
                'similar_page_pairs': high_similarity_pairs[:50],  # Limiter pour la performance
                'top_similarity_scores': sorted(similarity_scores, reverse=True)[:10]
            }
            
            print(f"üìä R√©sultats similarit√©:")
            print(f"  - Paires tr√®s similaires (>{similarity_threshold}): {len(high_similarity_pairs)}")
            print(f"  - Score moyen: {analysis['avg_similarity']:.3f}")
            
            return analysis
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse de similarit√©: {e}")
            return None

    def analyze_content_clusters(self, csv_path, url_filter=None):
        """Analyse les clusters de contenu"""
        try:
            rows, fieldnames = self.load_csv_file(csv_path)
            if not rows:
                return None
                
            print(f"üóÇÔ∏è  Analyse des clusters de contenu ({len(rows):,} pages)")
            
            # Identifier les colonnes
            url_col = cluster_col = None
            for col in fieldnames:
                col_lower = col.lower()
                if any(term in col_lower for term in ['url', 'address', 'page']):
                    url_col = col
                elif any(term in col_lower for term in ['cluster', 'group', 'category']):
                    cluster_col = col
            
            if not url_col:
                print("‚ùå Colonne URL non trouv√©e")
                return None
            
            # Grouper par clusters
            clusters = {}
            for row in rows:
                url = row.get(url_col, '')
                cluster_id = row.get(cluster_col, 'unclustered') if cluster_col else 'all_pages'
                
                # Appliquer le filtre
                if url_filter and not url.startswith(url_filter):
                    continue
                
                if cluster_id not in clusters:
                    clusters[cluster_id] = []
                clusters[cluster_id].append(url)
            
            # Analyser la distribution des clusters
            min_cluster_size = self.config.get('semantic_analysis', {}).get('min_cluster_size', 3)
            meaningful_clusters = {k: v for k, v in clusters.items() if len(v) >= min_cluster_size}
            
            analysis = {
                'total_clusters': len(clusters),
                'meaningful_clusters': len(meaningful_clusters),
                'total_clustered_pages': sum(len(pages) for pages in clusters.values()),
                'avg_cluster_size': sum(len(pages) for pages in clusters.values()) / len(clusters) if clusters else 0,
                'cluster_distribution': {k: len(v) for k, v in meaningful_clusters.items()},
                'largest_clusters': dict(sorted(meaningful_clusters.items(), key=lambda x: len(x[1]), reverse=True)[:10])
            }
            
            print(f"üìä R√©sultats clustering:")
            print(f"  - Clusters totaux: {analysis['total_clusters']}")
            print(f"  - Clusters significatifs (‚â•{min_cluster_size}): {analysis['meaningful_clusters']}")
            print(f"  - Taille moyenne: {analysis['avg_cluster_size']:.1f} pages/cluster")
            
            return analysis
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse des clusters: {e}")
            return None

    def generate_semantic_linking_recommendations(self, semantic_data):
        """G√©n√®re des recommandations de maillage bas√©es sur l'analyse s√©mantique"""
        recommendations = {
            'missing_internal_links': [],
            'cluster_orphans': [],
            'cross_cluster_opportunities': [],
            'content_gap_analysis': []
        }
        
        # Analyser les pages similaires sans liens
        if 'similar_pages' in semantic_data:
            similar_pairs = semantic_data['similar_pages'].get('similar_page_pairs', [])
            
            for i, pair in enumerate(similar_pairs[:20]):  # Limiter √† 20 recommandations
                recommendations['missing_internal_links'].append({
                    'source_page': pair['source'],
                    'target_page': pair['target'],
                    'similarity_score': pair['similarity'],
                    'recommendation': f"Lien √©ditorial recommand√© (similarit√©: {pair['similarity']:.3f})",
                    'priority': 'high' if pair['similarity'] > 0.9 else 'medium'
                })
        
        # Analyser les clusters pour identifier les opportunit√©s
        if 'content_clusters' in semantic_data:
            cluster_dist = semantic_data['content_clusters'].get('cluster_distribution', {})
            
            for cluster_id, size in cluster_dist.items():
                if size >= 5:  # Clusters avec au moins 5 pages
                    recommendations['cross_cluster_opportunities'].append({
                        'cluster': cluster_id,
                        'pages_count': size,
                        'recommendation': f"Cr√©er une page pilier pour le cluster '{cluster_id}' ({size} pages)",
                        'priority': 'high' if size > 10 else 'medium'
                    })
        
        print(f"üí° Recommandations s√©mantiques g√©n√©r√©es:")
        print(f"  - Liens manquants: {len(recommendations['missing_internal_links'])}")
        print(f"  - Opportunit√©s inter-clusters: {len(recommendations['cross_cluster_opportunities'])}")
        
        return recommendations

    def generate_network_data(self, internal_links, column_mapping):
        """G√©n√®re les donn√©es pour le graphique de r√©seau du maillage interne"""
        nodes = {}
        edges = []
        
        # Ne prendre que les liens √©ditoriaux pour le graphique
        editorial_links = [link for link in internal_links if not link.get('is_mechanical', False)]
        
        source_col = column_mapping.get('source')
        dest_col = column_mapping.get('dest')
        anchor_col = column_mapping.get('anchor')
        
        if not source_col or not dest_col:
            return {'nodes': [], 'edges': []}
        
        # Compter les liens entrants pour chaque page
        inbound_count = {}
        outbound_count = {}
        
        for link in editorial_links:
            source = link.get(source_col, '').strip()
            dest = link.get(dest_col, '').strip()
            
            if source and dest and source != dest:  # √âviter les auto-liens
                # Compter les liens entrants
                inbound_count[dest] = inbound_count.get(dest, 0) + 1
                outbound_count[source] = outbound_count.get(source, 0) + 1
                
                # Ajouter les n≈ìuds
                nodes[source] = nodes.get(source, {'id': source, 'inbound': 0, 'outbound': 0})
                nodes[dest] = nodes.get(dest, {'id': dest, 'inbound': 0, 'outbound': 0})
        
        # Mettre √† jour les compteurs des n≈ìuds
        for url, count in inbound_count.items():
            if url in nodes:
                nodes[url]['inbound'] = count
        
        for url, count in outbound_count.items():
            if url in nodes:
                nodes[url]['outbound'] = count
        
        # Cr√©er les ar√™tes avec ancres
        for link in editorial_links:
            source = link.get(source_col, '').strip()
            dest = link.get(dest_col, '').strip()
            anchor = link.get(anchor_col, '').strip() if anchor_col else ''
            
            if source and dest and source != dest:
                edges.append({
                    'source': source,
                    'target': dest,
                    'anchor': anchor[:50] + '...' if len(anchor) > 50 else anchor  # Limiter la longueur
                })
        
        # Limiter le nombre de n≈ìuds pour la performance (garder les plus connect√©s)
        if len(nodes) > 100:
            # Trier par nombre total de connexions (entrants + sortants)
            sorted_nodes = sorted(nodes.values(), 
                                key=lambda x: x['inbound'] + x['outbound'], 
                                reverse=True)[:100]
            
            # Filtrer les n≈ìuds et edges
            kept_urls = {node['id'] for node in sorted_nodes}
            nodes = {url: data for url, data in nodes.items() if url in kept_urls}
            edges = [edge for edge in edges if edge['source'] in kept_urls and edge['target'] in kept_urls]
        
        # Simplifier les URLs pour l'affichage
        for node in nodes.values():
            try:
                parsed = urlparse(node['id'])
                # Garder seulement le chemin, sans le domaine
                display_path = parsed.path.rstrip('/')
                if not display_path:
                    display_path = '/'
                elif len(display_path) > 30:
                    display_path = display_path[:27] + '...'
                node['label'] = display_path
            except:
                node['label'] = node['id'][:30] + '...' if len(node['id']) > 30 else node['id']
        
        return {
            'nodes': list(nodes.values()),
            'edges': edges[:500]  # Limiter les ar√™tes pour la performance
        }

    def generate_html_report(self, analysis, website_url, source_file, url_filter=None):
        """G√©n√®re le rapport HTML"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"{self.config['export_path']}audit_report_{timestamp}.html"
        
        os.makedirs(self.config['export_path'], exist_ok=True)
        
        stats = analysis['stats']
        
        # Score de qualit√©
        quality_score = analysis.get('editorial_quality_score', 0)
        score_color = '#28a745' if quality_score >= 80 else '#ffc107' if quality_score >= 60 else '#dc3545'
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Audit de Maillage Interne - {website_url}</title>
            <style>
                body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 40px; line-height: 1.6; color: #333; background: #f8f9fa; }}
                .container {{ max-width: 1200px; margin: 0 auto; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; text-align: center; }}
                .meta {{ background: white; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #6c757d; }}
                .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
                .stat-card {{ background: white; padding: 20px; border-radius: 8px; border-left: 4px solid #007bff; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                .stat-card.quality {{ border-left-color: {score_color}; }}
                .stat-number {{ font-size: 2.5em; font-weight: bold; color: #007bff; }}
                .stat-number.quality {{ color: {score_color}; }}
                .stat-label {{ color: #6c757d; font-size: 0.9em; }}
                .warning {{ background: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .success {{ background: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .danger {{ background: #f8d7da; border-left: 4px solid #dc3545; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .section {{ background: white; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #e1e5e9; }}
                th {{ background-color: #f8f9fa; font-weight: 600; }}
                tr:hover {{ background-color: #f8f9fa; }}
                .url {{ word-break: break-all; max-width: 500px; font-family: monospace; font-size: 0.85em; }}
                .recommendations {{ background: #e3f2fd; padding: 20px; border-radius: 8px; border-left: 4px solid #2196f3; }}
                ul.orphan-list {{ max-height: 300px; overflow-y: auto; background: #f8f9fa; padding: 15px; border-radius: 5px; }}
                .progress-bar {{ width: 100%; height: 20px; background: #e9ecef; border-radius: 10px; overflow: hidden; margin: 10px 0; }}
                .progress-fill {{ height: 100%; background: linear-gradient(90deg, #dc3545 0%, #ffc107 50%, #28a745 100%); transition: width 0.3s; }}
                .keyword-cloud {{ display: flex; flex-wrap: wrap; gap: 10px; margin: 15px 0; }}
                .keyword-tag {{ background: #e9ecef; padding: 5px 10px; border-radius: 15px; font-size: 0.85em; }}
                .chart-container {{ display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }}
                .chart {{ background: white; padding: 15px; border-radius: 8px; text-align: center; }}
                .bar-chart {{ margin: 15px 0; }}
                .bar-item {{ margin: 8px 0; display: flex; align-items: center; }}
                .bar-label {{ min-width: 120px; font-size: 0.9em; margin-right: 10px; }}
                .bar-container {{ flex: 1; display: flex; align-items: center; }}
                .bar-fill {{ height: 20px; background: linear-gradient(90deg, #36A2EB, #4BC0C0); border-radius: 10px; margin-right: 8px; min-width: 2px; }}
                .bar-value {{ font-weight: bold; color: #333; min-width: 30px; }}
                .pie-chart {{ margin: 15px 0; }}
                .pie-item {{ margin: 8px 0; display: flex; align-items: center; }}
                .pie-color {{ width: 16px; height: 16px; border-radius: 50%; margin-right: 8px; }}
                .pie-label {{ font-size: 0.9em; }}
                
                /* Styles pour l'analyse s√©mantique */
                .semantic-analysis {{ margin: 20px 0; }}
                .pie-chart-semantic {{ margin: 15px 0; }}
                .pie-item-semantic {{ margin: 8px 0; display: flex; align-items: center; }}
                .pie-label-semantic {{ font-size: 0.9em; font-weight: 500; }}
                .themes-detail {{ margin: 15px 0; }}
                .theme-item {{ margin: 15px 0; padding: 10px; border-radius: 8px; background: #f8f9fa; }}
                .theme-header {{ display: flex; justify-content: space-between; align-items: center; margin-bottom: 8px; }}
                .theme-name {{ font-weight: bold; color: #333; }}
                .theme-count {{ font-size: 0.9em; color: #666; }}
                .theme-bar-container {{ width: 100%; height: 20px; background: #e9ecef; border-radius: 10px; margin-bottom: 8px; }}
                .theme-bar {{ height: 100%; border-radius: 10px; }}
                .theme-examples {{ display: flex; flex-wrap: wrap; gap: 5px; }}
                .anchor-example {{ background: #e3f2fd; padding: 2px 6px; border-radius: 12px; font-size: 0.8em; color: #1976d2; }}
                .anchor-more {{ font-style: italic; color: #666; font-size: 0.8em; }}
                
                /* Nuages de mots par th√®me */
                .word-clouds-container {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }}
                .word-cloud-theme {{ background: white; padding: 15px; border-radius: 8px; }}
                .word-cloud-mini {{ display: flex; flex-wrap: wrap; gap: 8px; margin-top: 10px; }}
                .word-mini {{ padding: 3px 8px; background: rgba(0,0,0,0.05); border-radius: 12px; font-weight: 500; }}
                
                /* Analyses de coh√©rence et opportunit√©s */
                .coherence-analysis, .opportunities-analysis {{ margin: 25px 0; padding: 20px; background: #f8f9fa; border-radius: 8px; }}
                #network-graph {{ width: 100%; height: 600px; border: 1px solid #e1e5e9; border-radius: 8px; background: white; }}
                .network-controls {{ margin: 15px 0; text-align: center; }}
                .network-controls button {{ margin: 0 5px; padding: 8px 16px; border: 1px solid #007bff; background: white; color: #007bff; border-radius: 4px; cursor: pointer; }}
                .network-controls button:hover {{ background: #007bff; color: white; }}
                .network-controls button.active {{ background: #007bff; color: white; }}
                .tooltip {{ position: absolute; background: rgba(0,0,0,0.8); color: white; padding: 8px; border-radius: 4px; font-size: 12px; pointer-events: none; z-index: 1000; }}
            </style>
            <script src="https://d3js.org/d3.v7.min.js"></script>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>Audit de maillage interne</h1>
                    <p><strong>Site:</strong> {website_url}</p>
                    <p><strong>Date:</strong> {datetime.now().strftime("%d/%m/%Y √† %H:%M")}</p>
                </div>
                
                <div class="meta">
                    <strong>Fichier source :</strong> {os.path.basename(source_file)}<br>
                    <strong>Script :</strong> Audit automatis√© de maillage interne v2.0"""
        
        if url_filter:
            html_content += f"""<br>
                    <strong>üéØ Filtre appliqu√©:</strong> URLs commen√ßant par {url_filter}"""
        
        html_content += f"""
                </div>
                
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{stats['total_pages']:,}</div>
                        <div class="stat-label">Pages analys√©es</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{stats['editorial_links']:,}</div>
                        <div class="stat-label">Liens √©ditoriaux</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{stats['editorial_ratio']:.1f}%</div>
                        <div class="stat-label">Ratio √©ditorial</div>
                    </div>
                    <div class="stat-card quality">
                        <div class="stat-number quality">{quality_score}/100</div>
                        <div class="stat-label">Score qualit√©</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{stats['avg_editorial_per_page']:.1f}</div>
                        <div class="stat-label">Liens √©ditoriaux/page</div>
                    </div>
                </div>
                
                <div class="section">
                    <h2>Score de qualit√© √©ditorial</h2>
                    <div class="progress-bar">
                        <div class="progress-fill" style="width: {quality_score}%"></div>
                    </div>
                    <p><strong>{quality_score}/100</strong> - """
        
        if quality_score >= 80:
            html_content += """<span style="color: #28a745;">Excellente qualit√©</span></p>"""
        elif quality_score >= 60:
            html_content += """<span style="color: #ffc107;">Qualit√© moyenne</span></p>"""
        else:
            html_content += """<span style="color: #dc3545;">Qualit√© √† am√©liorer</span></p>"""
        
        html_content += "</div>"
        
        # Graphique de r√©seau du maillage interne
        if 'network_data' in analysis and analysis['network_data']['nodes']:
            network_data = analysis['network_data']
            html_content += f"""
            <div class="section">
                <h2>Graphique du maillage interne</h2>
                <p>Visualisation interactive des liens √©ditoriaux entre les pages. La taille des n≈ìuds correspond au nombre de liens entrants.</p>
                
                <div class="network-controls">
                    <button onclick="resetZoom()" class="active">R√©initialiser vue</button>
                    <button onclick="toggleLabels()">Basculer libell√©s</button>
                    <button onclick="highlightOrphans()">Surligner orphelines</button>
                </div>
                
                <div id="network-graph"></div>
                
                <p style="margin-top: 15px; color: #6c757d; font-size: 0.9em;">
                    <strong>L√©gende :</strong> 
                    Taille = liens entrants | 
                    Vert = bien connect√© | 
                    Jaune = moyennement connect√© | 
                    Rouge = peu connect√©
                </p>
            </div>
            
            <script>
            // Donn√©es du r√©seau
            const networkData = {json.dumps(network_data, ensure_ascii=False)};
            
            // Configuration du graphique
            const width = 1160;
            const height = 600;
            const margin = {{top: 20, right: 20, bottom: 20, left: 20}};
            
            // Cr√©er le SVG
            const svg = d3.select("#network-graph")
                .append("svg")
                .attr("width", width)
                .attr("height", height);
            
            const g = svg.append("g");
            
            // Zoom
            const zoom = d3.zoom()
                .scaleExtent([0.1, 3])
                .on("zoom", function(event) {{
                    g.attr("transform", event.transform);
                }});
            
            svg.call(zoom);
            
            // √âchelles pour la taille et couleur des n≈ìuds
            const maxInbound = d3.max(networkData.nodes, d => d.inbound) || 1;
            const radiusScale = d3.scaleSqrt()
                .domain([0, maxInbound])
                .range([4, 25]);
            
            const colorScale = d3.scaleLinear()
                .domain([0, maxInbound * 0.3, maxInbound * 0.7, maxInbound])
                .range(['#dc3545', '#ffc107', '#28a745', '#007bff']);
            
            // Simulation de forces
            const simulation = d3.forceSimulation(networkData.nodes)
                .force("link", d3.forceLink(networkData.edges).id(d => d.id).distance(80))
                .force("charge", d3.forceManyBody().strength(-300))
                .force("center", d3.forceCenter(width / 2, height / 2))
                .force("collision", d3.forceCollide().radius(d => radiusScale(d.inbound) + 2));
            
            // Cr√©er les liens
            const links = g.append("g")
                .selectAll("line")
                .data(networkData.edges)
                .enter().append("line")
                .attr("stroke", "#999")
                .attr("stroke-opacity", 0.6)
                .attr("stroke-width", 1);
            
            // Cr√©er les n≈ìuds
            const nodes = g.append("g")
                .selectAll("circle")
                .data(networkData.nodes)
                .enter().append("circle")
                .attr("r", d => radiusScale(d.inbound))
                .attr("fill", d => colorScale(d.inbound))
                .attr("stroke", "#fff")
                .attr("stroke-width", 1.5)
                .call(d3.drag()
                    .on("start", dragstarted)
                    .on("drag", dragged)
                    .on("end", dragended));
            
            // Labels des n≈ìuds
            const labels = g.append("g")
                .selectAll("text")
                .data(networkData.nodes)
                .enter().append("text")
                .text(d => d.label)
                .attr("font-size", "10px")
                .attr("text-anchor", "middle")
                .attr("dy", ".35em")
                .attr("fill", "#333")
                .style("pointer-events", "none")
                .style("opacity", 0.8);
            
            // Tooltip
            const tooltip = d3.select("body").append("div")
                .attr("class", "tooltip")
                .style("opacity", 0);
            
            // Events pour les n≈ìuds
            nodes.on("mouseover", function(event, d) {{
                    tooltip.transition().duration(200).style("opacity", .9);
                    tooltip.html(`
                        <strong>${{d.label}}</strong><br/>
                        Liens entrants: ${{d.inbound}}<br/>
                        Liens sortants: ${{d.outbound}}<br/>
                        <small>${{d.id}}</small>
                    `)
                    .style("left", (event.pageX + 10) + "px")
                    .style("top", (event.pageY - 28) + "px");
                }})
                .on("mouseout", function(d) {{
                    tooltip.transition().duration(500).style("opacity", 0);
                }});
            
            // Animation de la simulation
            simulation.on("tick", () => {{
                links
                    .attr("x1", d => d.source.x)
                    .attr("y1", d => d.source.y)
                    .attr("x2", d => d.target.x)
                    .attr("y2", d => d.target.y);
                
                nodes
                    .attr("cx", d => d.x)
                    .attr("cy", d => d.y);
                
                labels
                    .attr("x", d => d.x)
                    .attr("y", d => d.y);
            }});
            
            // Fonctions de contr√¥le
            let labelsVisible = true;
            let orphansHighlighted = false;
            
            function resetZoom() {{
                svg.transition().duration(750).call(
                    zoom.transform,
                    d3.zoomIdentity
                );
            }}
            
            function toggleLabels() {{
                labelsVisible = !labelsVisible;
                labels.style("opacity", labelsVisible ? 0.8 : 0);
                
                // Mettre √† jour le bouton
                d3.select('button:nth-child(2)')
                    .classed('active', labelsVisible);
            }}
            
            function highlightOrphans() {{
                orphansHighlighted = !orphansHighlighted;
                
                nodes.attr("stroke", d => {{
                    if (orphansHighlighted && d.inbound === 0) {{
                        return "#ff0000";
                    }}
                    return "#fff";
                }})
                .attr("stroke-width", d => {{
                    if (orphansHighlighted && d.inbound === 0) {{
                        return 3;
                    }}
                    return 1.5;
                }});
                
                // Mettre √† jour le bouton
                d3.select('button:nth-child(3)')
                    .classed('active', orphansHighlighted);
            }}
            
            // Fonctions de drag
            function dragstarted(event, d) {{
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }}
            
            function dragged(event, d) {{
                d.fx = event.x;
                d.fy = event.y;
            }}
            
            function dragended(event, d) {{
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }}
            </script>
            """
        
        # Analyses de qualit√© du contenu
        if 'content_quality' in analysis:
            content_analysis = analysis['content_quality']
            
            # Analyse du nombre de mots
            if 'word_analysis' in content_analysis:
                word_data = content_analysis['word_analysis']
                html_content += f"""
                <div class="section">
                    <h2>Qualit√© du contenu</h2>
                    <h3>Analyse du nombre de mots</h3>
                    
                    <div class="chart-container">
                        <div class="chart">
                            <h4>R√©partition</h4>
                            <p>Total : <strong>{word_data['total_pages']:,}</strong> pages</p>
                            <p>üìà Moyenne: <strong>{word_data['avg_words']:.0f}</strong> mots</p>
                            <p>üìä M√©diane: <strong>{word_data['median_words']:.0f}</strong> mots</p>
                        </div>
                        <div class="chart">
                            <h4>Classification</h4>
                            <p>Contenu thin (&lt;300) : <strong>{len(word_data['thin_content'])}</strong></p>
                            <p>Contenu qualit√© (300-1500) : <strong>{len(word_data['quality_content'])}</strong></p>
                            <p>Contenu riche (&gt;1500) : <strong>{len(word_data['rich_content'])}</strong></p>
                        </div>
                    </div>
                """
                
                # Afficher les pages thin content comme recommandations
                if word_data['thin_content']:
                    html_content += """
                    <div class="warning">
                        <h4>Pages avec contenu thin (&lt; 300 mots)</h4>
                        <p>Ces pages ont peu de contenu et devraient √™tre √©vit√©es pour le maillage entrant ou enrichies :</p>
                        <ul>
                    """
                    for page in word_data['thin_content'][:10]:  # Limiter √† 10
                        html_content += f"<li><strong>{page['word_count']} mots</strong> - {page['url']}</li>"
                    if len(word_data['thin_content']) > 10:
                        html_content += f"<li><em>... et {len(word_data['thin_content']) - 10} autres pages</em></li>"
                    html_content += "</ul></div>"
                
                html_content += "</div>"
            
            # Analyse de coh√©rence Title/H1
            if 'title_h1_coherence' in content_analysis:
                coherence_data = content_analysis['title_h1_coherence']
                html_content += f"""
                <div class="section">
                    <h2>Coh√©rence title / H1</h2>
                    
                    <div class="chart-container">
                        <div class="chart">
                            <h4>Statistiques</h4>
                            <p>Pages analys√©es : <strong>{coherence_data['total_pages_with_both']:,}</strong></p>
                            <p>‚úÖ Identiques: <strong>{len(coherence_data['identical'])}</strong></p>
                            <p>üü° Similaires: <strong>{len(coherence_data['similar'])}</strong></p>
                            <p>üî¥ Diff√©rents: <strong>{len(coherence_data['different'])}</strong></p>
                        </div>
                        <div class="chart">
                            <h4>Probl√®mes d√©tect√©s</h4>
                            <p>‚ùå H1 manquant: <strong>{len(coherence_data['missing_h1'])}</strong></p>
                            <p>‚ùå Title manquant: <strong>{len(coherence_data['missing_title'])}</strong></p>
                        </div>
                    </div>
                """
                
                # Afficher les incoh√©rences
                if coherence_data['different']:
                    html_content += """
                    <div class="warning">
                        <h4>Pages avec title et H1 tr√®s diff√©rents</h4>
                        <p>Ces pages ont une incoh√©rence qui peut nuire au SEO :</p>
                        <table>
                            <tr><th>URL</th><th>Title</th><th>H1</th></tr>
                    """
                    for page in coherence_data['different'][:5]:  # Limiter √† 5
                        html_content += f"""<tr>
                            <td class='url'>{page['url']}</td>
                            <td>{page['title'][:60]}{'...' if len(page['title']) > 60 else ''}</td>
                            <td>{page['h1'][:60]}{'...' if len(page['h1']) > 60 else ''}</td>
                        </tr>"""
                    html_content += "</table></div>"
                
                html_content += "</div>"
            
            # Analyse des pages de conversion
            if 'conversion_pages' in content_analysis:
                conversion_data = content_analysis['conversion_pages']
                html_content += f"""
                <div class="section">
                    <h2>Pages de conversion identifi√©es</h2>
                    <p>Ces pages sont cruciales pour votre business et doivent √™tre bien maill√©es !</p>
                    
                    <div class="success">
                        <p><strong>üéØ {conversion_data['total_conversion_pages']} pages de conversion trouv√©es</strong> 
                        ({conversion_data['conversion_rate_potential']:.1f}% du site)</p>
                    </div>
                    
                    <div class="chart-container">
                """
                
                # Afficher par cat√©gorie
                categories_with_pages = {k: v for k, v in conversion_data['by_category'].items() if v}
                for category, pages in categories_with_pages.items():
                    html_content += f"""
                    <div class="chart">
                        <h4>{category.title().lower().capitalize()}</h4>
                        <p><strong>{len(pages)} pages</strong></p>
                        <ul>
                    """
                    for page in pages[:3]:  # Afficher les 3 premi√®res
                        html_content += f"<li>{page['url']}</li>"
                    if len(pages) > 3:
                        html_content += f"<li><em>... et {len(pages) - 3} autres</em></li>"
                    html_content += "</ul></div>"
                
                html_content += """
                    </div>
                    
                    <div class="recommendations">
                        <h4>Recommandations pour les pages de conversion</h4>
                        <ul>
                            <li><strong>Maillage entrant renforc√©</strong> : Ces pages doivent recevoir plus de liens √©ditoriaux</li>
                            <li><strong>Ancres contextuelles</strong> : Utilisez des ancres qui expliquent la valeur ajout√©e</li>
                            <li><strong>Position strat√©gique</strong> : Placez les liens dans le contenu, pas seulement en navigation</li>
                            <li><strong>Pages de contenu vers conversion</strong> : Liez depuis vos articles vers ces pages</li>
                        </ul>
                    </div>
                </div>
                """
        
        # Qualit√© des ancres
        if 'anchor_quality' in analysis:
            anchor_quality = analysis['anchor_quality']
            html_content += f"""
            <div class="section">
                <h2>Qualit√© des ancres √©ditoriales</h2>
                <div class="chart-container">
                    <div class="chart">
                        <h4>R√©partition</h4>
                        <p>Bonne qualit√© : <strong>{len(anchor_quality.get('good_quality', []))}</strong></p>
                        <p>Trop courtes : <strong>{len(anchor_quality.get('too_short', []))}</strong></p>
                        <p>Trop longues : <strong>{len(anchor_quality.get('too_long', []))}</strong></p>
                        <p>Sur-optimis√©es : <strong>{len(anchor_quality.get('keyword_stuffed', []))}</strong></p>
                    </div>
                </div>
            """
            
            if anchor_quality.get('too_short'):
                html_content += """
                <div class="warning">
                    <h4>Ancres trop courtes (exemples)</h4>
                    <ul>
                """
                for item in anchor_quality['too_short'][:5]:
                    html_content += f"<li><strong>'{item['anchor']}'</strong> ‚Üí {item['dest']}</li>"
                html_content += "</ul></div>"
            
            if anchor_quality.get('keyword_stuffed'):
                html_content += """
                <div class="danger">
                    <h4>Ancres potentiellement sur-optimis√©es</h4>
                    <ul>
                """
                for item in anchor_quality['keyword_stuffed'][:5]:
                    html_content += f"<li><strong>'{item['anchor']}'</strong> ‚Üí {item['dest']}</li>"
                html_content += "</ul></div>"
            
            html_content += "</div>"
        
        # Distribution th√©matique
        if 'thematic_distribution' in analysis:
            thematic = analysis['thematic_distribution']
            html_content += """
            <div class="section">
                <h2>Distribution th√©matique</h2>
            """
            
            if thematic.get('top_anchor_keywords'):
                # Cr√©er des graphiques simples en barres avec CSS
                keywords_data = list(thematic['top_anchor_keywords'].items())[:10]  # Top 10
                max_count = max([count for _, count in keywords_data]) if keywords_data else 1
                
                html_content += """
                <div class="chart-container">
                    <div class="chart">
                        <h4>Mots-cl√©s principaux dans les ancres</h4>
                        <div class="bar-chart">
                """
                
                for keyword, count in keywords_data:
                    percentage = (count / max_count) * 100
                    html_content += f"""
                            <div class="bar-item">
                                <span class="bar-label">{keyword}</span>
                                <div class="bar-container">
                                    <div class="bar-fill" style="width: {percentage}%"></div>
                                    <span class="bar-value">{count}</span>
                                </div>
                            </div>
                    """
                
                html_content += """
                        </div>
                    </div>
                """
            
            if thematic.get('destination_categories'):
                categories_data = list(thematic['destination_categories'].items())
                total_categories = sum([count for _, count in categories_data]) if categories_data else 1
                
                html_content += """
                    <div class="chart">
                        <h4>Types de pages li√©es</h4>
                        <div class="pie-chart">
                """
                
                colors = ['#FF6384', '#36A2EB', '#FFCE56', '#4BC0C0', '#9966FF']
                for i, (category, count) in enumerate(categories_data):
                    percentage = (count / total_categories) * 100
                    color = colors[i % len(colors)]
                    html_content += f"""
                            <div class="pie-item">
                                <span class="pie-color" style="background-color: {color}"></span>
                                <span class="pie-label">{category}: {count} ({percentage:.1f}%)</span>
                            </div>
                    """
                
                html_content += """
                        </div>
                    </div>
                </div>
                """
            else:
                html_content += """
                </div>
                """
            
            # Afficher aussi le nuage de mots-cl√©s en compl√©ment
            if thematic.get('top_anchor_keywords'):
                html_content += """
                <h4>Nuage de mots-cl√©s</h4>
                <div class="keyword-cloud">
                """
                for keyword, count in list(thematic['top_anchor_keywords'].items())[:15]:
                    html_content += f"""<span class="keyword-tag">{keyword} ({count})</span>"""
                html_content += "</div>"
            
            html_content += "</div>"
        
        # Analyse s√©mantique avanc√©e CamemBERT
        if 'advanced_semantic' in analysis and analysis['advanced_semantic']:
            html_content += self.generate_semantic_analysis_section(analysis['advanced_semantic'])
        
        # Pages les plus li√©es
        if analysis['most_linked_pages']:
            html_content += """
            <div class="section">
                <h2>Pages les plus li√©es (liens entrants √©ditoriaux)</h2>
                <table>
                    <tr><th>URL</th><th>Liens entrants</th></tr>
            """
            for url, count in list(analysis['most_linked_pages'].items())[:15]:
                html_content += f"<tr><td class='url'>{url}</td><td><strong>{count}</strong></td></tr>"
            html_content += "</table></div>"
        
        # Pages orphelines
        if analysis['orphan_pages']:
            html_content += f"""
            <div class="section">
                <h2>Pages orphelines</h2>
                <div class="warning">
                    <p><strong>{len(analysis['orphan_pages'])} pages sans liens entrants √©ditoriaux:</strong></p>
                    <ul class="orphan-list">
            """
            for orphan in analysis['orphan_pages'][:30]:
                html_content += f"<li class='url'>{orphan}</li>"
            if len(analysis['orphan_pages']) > 30:
                html_content += f"<li><em>... et {len(analysis['orphan_pages']) - 30} autres</em></li>"
            html_content += "</ul></div></div>"
        
        # Ancres sur-optimis√©es
        if analysis['over_optimized_anchors']:
            html_content += """
            <div class="section">
                <h2>Ancres potentiellement sur-optimis√©es</h2>
                <div class="warning">
                    <table>
                        <tr><th>Ancre</th><th>Occurrences</th></tr>
            """
            for anchor, count in list(analysis['over_optimized_anchors'].items())[:10]:
                html_content += f"<tr><td>{anchor}</td><td><strong>{count}</strong></td></tr>"
            html_content += "</table></div></div>"
        
        # Recommandations personnalis√©es
        html_content += f"""
            <div class="recommendations">
                <h2>Recommandations prioritaires</h2>
                <ul>
        """
        
        # Recommandations dynamiques bas√©es sur l'analyse
        if len(analysis['orphan_pages']) > 0:
            html_content += f"<li><strong>Pages orphelines ({len(analysis['orphan_pages'])}):</strong> Cr√©er des liens √©ditoriaux contextuels depuis vos contenus les plus populaires</li>"
        
        if stats['editorial_ratio'] < 50:
            html_content += f"<li><strong>Ratio √©ditorial faible ({stats['editorial_ratio']:.1f}%):</strong> Augmenter les liens √©ditoriaux dans vos contenus</li>"
        
        if analysis.get('anchor_quality', {}).get('too_short'):
            html_content += f"<li><strong>Ancres trop courtes:</strong> Am√©liorer {len(analysis['anchor_quality']['too_short'])} ancres avec des descriptions plus pr√©cises</li>"
        
        if stats['avg_editorial_per_page'] < 2:
            html_content += f"<li><strong>Maillage insuffisant:</strong> Viser 2-3 liens √©ditoriaux minimum par page (actuellement {stats['avg_editorial_per_page']:.1f})</li>"
        
        html_content += """
                    <li><strong>Ancres naturelles:</strong> Utiliser des ancres descriptives qui d√©crivent le contenu de destination</li>
                    <li><strong>Contexte √©ditorial:</strong> Int√©grer les liens dans le corps du texte plut√¥t qu'en navigation</li>
                    <li><strong>Diversit√© th√©matique:</strong> Varier les ancres pour √©viter la sur-optimisation</li>
                </ul>
            </div>
        </div>
        </body>
        </html>
        """
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        # G√©n√©rer aussi un export CSV des recommandations
        csv_export_file = self.generate_csv_export(analysis, website_url, source_file, url_filter)
        
        return report_file

    def generate_csv_export(self, analysis, website_url, source_file, url_filter=None):
        """G√©n√®re un export CSV des recommandations"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_file = f"{self.config['export_path']}recommendations_{timestamp}.csv"
        
        try:
            with open(csv_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                
                # En-t√™tes
                writer.writerow([
                    'Type', 'Priorit√©', 'URL', 'Probl√®me', 'Recommandation', 'D√©tails'
                ])
                
                # Pages orphelines
                for orphan in analysis.get('orphan_pages', []):
                    writer.writerow([
                        'Page orpheline',
                        'Haute',
                        orphan,
                        'Aucun lien entrant √©ditorial',
                        'Cr√©er des liens √©ditoriaux contextuels',
                        'Page sans maillage interne √©ditorial'
                    ])
                
                # Ancres probl√©matiques
                if 'anchor_quality' in analysis:
                    anchor_quality = analysis['anchor_quality']
                    
                    # Ancres trop courtes
                    for item in anchor_quality.get('too_short', []):
                        writer.writerow([
                            'Ancre d√©faillante',
                            'Moyenne',
                            item['dest'],
                            f"Ancre trop courte: '{item['anchor']}'",
                            'Utiliser une ancre plus descriptive',
                            f"Ancre actuelle: '{item['anchor']}'"
                        ])
                    
                    # Ancres sur-optimis√©es
                    for item in anchor_quality.get('keyword_stuffed', []):
                        writer.writerow([
                            'Ancre sur-optimis√©e',
                            'Haute',
                            item['dest'],
                            f"Ancre potentiellement sur-optimis√©e: '{item['anchor']}'",
                            'Diversifier avec des expressions naturelles',
                            'Risque de p√©nalit√© SEO'
                        ])
                    
                    # Ancres URL
                    for item in anchor_quality.get('url_anchors', []):
                        writer.writerow([
                            'Ancre non-optimis√©e',
                            'Moyenne',
                            item['dest'],
                            f"URL utilis√©e comme ancre: '{item['anchor']}'",
                            'Remplacer par une ancre descriptive',
                            'Les URLs ne sont pas des ancres optimales'
                        ])
                
                # Ancres r√©p√©titives
                for anchor, count in analysis.get('over_optimized_anchors', {}).items():
                    writer.writerow([
                        'Ancre r√©p√©titive',
                        'Moyenne',
                        'Multiple',
                        f"Ancre utilis√©e {count} fois: '{anchor}'",
                        'Diversifier les variantes s√©mantiques',
                        f'R√©p√©tition excessive ({count} occurrences)'
                    ])
                
                # Recommandations g√©n√©rales bas√©es sur les stats
                stats = analysis.get('stats', {})
                
                if stats.get('editorial_ratio', 0) < 50:
                    writer.writerow([
                        'Strat√©gie globale',
                        'Haute',
                        website_url or 'Site entier',
                        f"Ratio √©ditorial faible ({stats['editorial_ratio']:.1f}%)",
                        'Augmenter les liens √©ditoriaux dans les contenus',
                        'Moins de 50% de liens √©ditoriaux'
                    ])
                
                if stats.get('avg_editorial_per_page', 0) < 2:
                    writer.writerow([
                        'Densit√© de maillage',
                        'Moyenne',
                        website_url or 'Site entier',
                        f"Maillage insuffisant ({stats['avg_editorial_per_page']:.1f} liens/page)",
                        'Viser 2-3 liens √©ditoriaux minimum par page',
                        'Maillage interne sous-optimal'
                    ])
            
            print(f"üìä Export CSV g√©n√©r√©: {csv_file}")
            return csv_file
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la g√©n√©ration du CSV: {e}")
            return None

    def show_config(self):
        """Affiche et permet de modifier la configuration"""
        print(f"\n‚öôÔ∏è  CONFIGURATION ACTUELLE")
        print("="*50)
        print(f"üìç Screaming Frog: {self.config['screaming_frog_path']}")
        print(f"üìÅ Dossier export: {self.config['export_path']}")
        print(f"üìè Longueur min ancre: {self.config['min_anchor_length']}")
        print(f"üö´ Extensions ignor√©es: {', '.join(self.config['ignore_extensions'])}")
        
        modify = input("\nModifier la configuration ? (o/N): ").lower()
        if modify == 'o':
            # Ici on pourrait ajouter la modification de config
            print("Modification de config pas encore impl√©ment√©e")

    def run(self):
        """Lance l'application principale"""
        while True:
            choice = self.show_menu()
            
            if choice == '1':
                # Nouveau crawl
                website_url = input("\nüåê URL du site √† crawler: ").strip()
                if not website_url:
                    print("‚ùå URL requise")
                    continue
                
                # Option de filtrage par pr√©fixe d'URL
                print("\nüéØ FILTRAGE D'URLs (optionnel)")
                print("Vous pouvez limiter l'analyse √† une section sp√©cifique du site.")
                print("Exemple: https://monsite.com/blog/ pour ne garder que les pages du blog")
                
                url_filter = input("üîç Pr√©fixe d'URL √† conserver (vide = tout le site): ").strip()
                if url_filter and not url_filter.startswith('http'):
                    print("‚ö†Ô∏è  Le filtre doit commencer par http:// ou https://")
                    url_filter = None
                
                if url_filter:
                    print(f"‚úÖ Filtrage activ√©: seules les URLs commen√ßant par '{url_filter}' seront analys√©es")
                
                csv_file = self.run_new_crawl(website_url, url_filter)
                if csv_file:
                    input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour lancer l'analyse...")
                    self.analyze_csv(csv_file, website_url, url_filter)
                
            elif choice == '2':
                # Analyser CSV existant
                csv_files = self.list_existing_csvs()
                if not csv_files:
                    input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour continuer...")
                    continue
                
                while True:
                    try:
                        file_choice = input(f"\nChoisir un fichier (1-{len(csv_files)}) ou 'r' pour retour: ").strip()
                        if file_choice.lower() == 'r':
                            break
                        
                        file_index = int(file_choice) - 1
                        if 0 <= file_index < len(csv_files):
                            selected_file = csv_files[file_index]
                            website_url = input("üåê URL du site (optionnel): ").strip() or None
                            
                            # Option de filtrage pour CSV existant aussi
                            url_filter = None
                            if website_url:
                                print("\nüéØ FILTRAGE D'URLs (optionnel)")
                                url_filter = input("üîç Pr√©fixe d'URL √† conserver (vide = tout): ").strip() or None
                                if url_filter and not url_filter.startswith('http'):
                                    print("‚ö†Ô∏è  Le filtre doit commencer par http:// ou https://")
                                    url_filter = None
                            
                            self.analyze_csv(selected_file, website_url, url_filter)
                            break
                        else:
                            print("‚ùå Num√©ro invalide")
                    except ValueError:
                        print("‚ùå Veuillez entrer un num√©ro")
                
            elif choice == '3':
                # Lister CSV
                self.list_existing_csvs()
                input("\n‚è∏Ô∏è  Appuyez sur Entr√©e pour continuer...")
                
            elif choice == '4':
                # Configuration
                self.show_config()
                
            elif choice == '5':
                # Quitter
                print("\nüëã Au revoir !")
                break
            
            print("\n" + "="*50)

if __name__ == "__main__":
    auditor = CompleteLinkAuditor()
    auditor.run()