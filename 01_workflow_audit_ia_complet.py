#!/usr/bin/env python3
"""
WORKFLOW FINAL INTELLIGENT : IA + Screaming Frog + Analyse SÃ©mantique
Combinaison parfaite de l'analyse IA de contenu et du crawl Screaming Frog
"""

import subprocess
import os
import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Optional
from ext_detecteur_contenu_ia import IntelligentContentDetector
import json
import time
import glob

class FinalIntelligentWorkflow:
    """Workflow final combinant IA + Screaming Frog + Analyse sÃ©mantique"""
    
    def __init__(self):
        self.detector = IntelligentContentDetector()
        self.xpath_content = ""
        self.xpath_links = ""
        self.ai_analysis = {}
        
    def fetch_and_parse_sitemap(self, sitemap_url: str) -> List[str]:
        """RÃ©cupÃ¨re et analyse un sitemap XML pour extraire les URLs"""
        try:
            import xml.etree.ElementTree as ET

            print(f"   ğŸ“¥ TÃ©lÃ©chargement du sitemap...")
            response = requests.get(sitemap_url, timeout=30)
            response.raise_for_status()

            # Parser le XML
            root = ET.fromstring(response.content)

            urls = []
            # GÃ©rer les namespaces XML
            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

            # Chercher les Ã©lÃ©ments <loc> (URLs)
            for loc in root.findall('.//sm:loc', ns) or root.findall('.//loc'):
                if loc.text:
                    url = loc.text.strip()
                    if url and url.startswith('http'):
                        urls.append(url)

            # Si pas d'URLs trouvÃ©es, essayer sans namespace
            if not urls:
                for loc in root.findall('.//loc'):
                    if loc.text:
                        url = loc.text.strip()
                        if url and url.startswith('http'):
                            urls.append(url)

            return urls

        except Exception as e:
            print(f"   âŒ Erreur sitemap: {e}")
            return []

    def run_complete_workflow(self, website_url: str, section_filter: str = "", max_pages: str = "", sample_urls: Optional[List[str]] = None) -> str:
        """Workflow complet : IA â†’ SF Crawl â†’ Filtrage intelligent â†’ Analyse sÃ©mantique"""
        print(f"ğŸš€ WORKFLOW INTELLIGENT FINAL")
        print(f"Site: {website_url}")
        if section_filter:
            print(f"Section: {section_filter}")
        if max_pages:
            print(f"Limite: {max_pages} pages")
        print("=" * 70)
        
        # Ã‰tape 1 : Analyse IA de la structure
        print(f"\nğŸ¤– Ã‰TAPE 1: Analyse IA de la structure de contenu")
        print("-" * 50)
        
        structure_analysis = self.detector.run_intelligent_workflow(website_url, section_filter, sample_urls)
        
        if not structure_analysis.get('success'):
            print("âš ï¸  Ã‰chec de l'analyse IA, utilisation des valeurs par dÃ©faut")
            self.ai_analysis = {}
            content_zones = {}
        else:
            self.ai_analysis = structure_analysis.get('ai_analysis', {})
            content_zones = self.ai_analysis.get('content_zones', {})

        self.xpath_content = content_zones.get('main_content_xpath', '//main')
        self.xpath_links = content_zones.get('editorial_links_xpath', '//main//a')

        print(f"   âœ… XPath contenu dÃ©tectÃ©: {self.xpath_content}")
        print(f"   âœ… XPath liens Ã©ditoriaux: {self.xpath_links}")
        
        # Ã‰tape 2 : VÃ©rification des donnÃ©es Screaming Frog
        print(f"\nğŸ•·ï¸  Ã‰TAPE 2: VÃ©rification des donnÃ©es Screaming Frog")
        print("-" * 50)
        
        sf_data_available = self.check_screaming_frog_data()
        if not sf_data_available:
            print("   âš ï¸  Aucune donnÃ©e SF rÃ©cente trouvÃ©e, lancement du crawl...")
            sf_success = self.run_screaming_frog_crawl(website_url, section_filter, max_pages)
            if not sf_success:
                print("âŒ Ã‰chec du crawl Screaming Frog")
                return ""
        else:
            print("   âœ… DonnÃ©es Screaming Frog disponibles, utilisation des donnÃ©es existantes")

        # Temporaire: forcer l'utilisation des donnÃ©es existantes
        sf_data_available = True
        
        # Ã‰tape 3 : Filtrage intelligent avec l'IA
        print(f"\nğŸ§  Ã‰TAPE 3: Filtrage intelligent des liens avec IA")
        print("-" * 50)
        
        filtered_report = self.intelligent_post_processing(website_url, section_filter)
        if not filtered_report:
            print("âŒ Ã‰chec du post-traitement intelligent")
            return ""
        
        # Ã‰tape 4 : Analyse sÃ©mantique finale
        print(f"\nğŸ“Š Ã‰TAPE 4: Lancement de l'analyse sÃ©mantique")
        print("-" * 50)
        
        semantic_success = self.launch_semantic_analysis(filtered_report)
        
        print(f"\nâœ… WORKFLOW INTELLIGENT TERMINÃ‰ AVEC SUCCÃˆS!")
        print(f"ğŸ“‹ RÃ©sumÃ©:")
        print(f"   ğŸ¤– Analyse IA: âœ…")
        print(f"   ğŸ•·ï¸  Crawl SF: âœ…")  
        print(f"   ğŸ§  Filtrage intelligent: âœ…")
        print(f"   ğŸ“Š Analyse sÃ©mantique: {'âœ…' if semantic_success else 'âš ï¸'}")
        
        return filtered_report
    
    def check_screaming_frog_data(self) -> bool:
        """VÃ©rifier si des donnÃ©es Screaming Frog rÃ©centes sont disponibles"""
        sf_files = [
            "./exports/tous_les_liens_sortants.csv",
            "./exports/all_outlinks.csv"
        ]
        
        for file_path in sf_files:
            if os.path.exists(file_path):
                # VÃ©rifier que le fichier n'est pas vide et rÃ©cent
                stat = os.stat(file_path)
                if stat.st_size > 1000:  # Plus de 1KB
                    file_age_hours = (time.time() - stat.st_mtime) / 3600
                    if file_age_hours < 24*30:  # Moins de 30 jours (temporaire pour test)
                        print(f"   ğŸ“„ Fichier trouvÃ©: {file_path} ({stat.st_size/1024/1024:.1f}MB)")
                        return True
        
        return False
    
    def run_screaming_frog_crawl(self, website_url: str, section_filter: str = "", max_pages: str = "") -> bool:
        """ExÃ©cuter le crawl Screaming Frog optimisÃ© avec filtres"""

        # Nettoyer les anciens fichiers (supprimer donnÃ©es SF)
        self.cleanup_old_files(preserve_sf_data=False)

        # DÃ©tecter le systÃ¨me et choisir le bon chemin
        import platform
        import os

        system = platform.system()
        if system == "Windows":
            sf_path = "C:\\Program Files (x86)\\Screaming Frog SEO Spider\\ScreamingFrogSEOSpiderCli.exe"
        elif system == "Darwin":  # macOS
            sf_path = "/Applications/Screaming Frog SEO Spider.app/Contents/MacOS/ScreamingFrogSEOSpiderCli"
        else:  # Linux et autres
            # DÃ©tecter WSL
            is_wsl = False
            try:
                if os.path.exists('/mnt/c'):
                    is_wsl = True
                elif os.path.exists('/proc/version'):
                    with open('/proc/version', 'r') as f:
                        if 'microsoft' in f.read().lower():
                            is_wsl = True
            except:
                pass

            if is_wsl:
                sf_path = "/mnt/c/Program Files (x86)/Screaming Frog SEO Spider/ScreamingFrogSEOSpiderCli.exe"
            else:
                sf_path = "/usr/bin/screamingfrogseospider"

        # Modifier l'URL de dÃ©part si un filtre de section est spÃ©cifiÃ©
        crawl_url = website_url
        if section_filter:
            # Construire l'URL avec la section pour limiter le crawl
            from urllib.parse import urljoin
            crawl_url = urljoin(website_url.rstrip('/') + '/', section_filter.lstrip('/'))
            print(f"   ğŸ” Crawl limitÃ© Ã  la section: {crawl_url}")

        command = [
            sf_path,
            '-headless',
            '-crawl', crawl_url,
            '--output-folder', './exports/',
            '--export-format', 'csv'
        ]

        # Ajouter des filtres pour limiter le crawl Ã  la section
        if section_filter:
            # Inclure seulement les URLs de la section
            include_pattern = f"*{section_filter}*"
            command.extend(['--include', include_pattern])
            print(f"   ğŸ“‹ Pattern d'inclusion: {include_pattern}")

            # Limiter la profondeur pour Ã©viter de sortir de la section
            command.extend(['--crawl-depth', '3'])

        # Utiliser la config IA si elle existe (dÃ©sactivÃ© temporairement pour debug)
        # if os.path.exists('./sf_content_config.xml'):
        #     command.extend(['-config', './sf_content_config.xml'])
        #     command.extend(['--bulk-export', 'Links:All Outlinks,All Inlinks,Custom:MainContent,Custom:EditorialLinks,Custom:ContentZone,Page Titles,H1-1,Word Count'])
        # else:
        command.extend(['--bulk-export', 'All Outlinks,Page Titles,H1-1,Word Count'])

        command.append('--overwrite')
            
        # Ajouter la limite de pages si spÃ©cifiÃ©e  
        if max_pages and max_pages.isdigit():
            print(f"   ğŸ“Š Limite de pages: {max_pages}")
            # SF ne supporte pas directement la limite via CLI
            # On peut essayer d'ajouter des paramÃ¨tres personnalisÃ©s mais c'est limitÃ©
        
        print(f"   ğŸ”„ Lancement du crawl Screaming Frog...")
        print(f"   â±ï¸  Cela peut prendre quelques minutes...")
        
        try:
            result = subprocess.run(command, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                print(f"   âœ… Crawl Screaming Frog terminÃ© avec succÃ¨s")
                # VÃ©rifier que les fichiers ont Ã©tÃ© crÃ©Ã©s
                expected_files = ["./exports/all_outlinks.csv", "./exports/page_titles.csv"]
                files_found = [f for f in expected_files if os.path.exists(f)]

                if files_found:
                    print(f"   ğŸ“„ Fichiers gÃ©nÃ©rÃ©s: {', '.join([os.path.basename(f) for f in files_found])}")
                    return True
                else:
                    print(f"   âš ï¸  Crawl terminÃ© mais fichiers attendus non trouvÃ©s")
                    # Lister les fichiers prÃ©sents pour debug
                    if os.path.exists("./exports/"):
                        existing_files = os.listdir("./exports/")
                        if existing_files:
                            print(f"   ğŸ“‚ Fichiers prÃ©sents: {existing_files}")
                    return False
            else:
                print(f"   âŒ Erreur crawl SF (code {result.returncode})")
                if result.stderr:
                    print(f"   ğŸ“„ Erreur: {result.stderr[:200]}...")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"   â° Timeout crawl SF (5 minutes dÃ©passÃ©es)")
            return False
        except Exception as e:
            print(f"   âŒ Erreur crawl SF: {e}")
            return False
    
    def cleanup_old_files(self, preserve_sf_data=False):
        """Nettoyer les anciens fichiers qui pourraient causer des conflits"""
        files_to_remove = [
            "./exports/editorial_links_filtered.csv",
            "./exports/editorial_links_intelligent.csv"
        ]
        
        # Seulement supprimer les donnÃ©es SF si explicitement demandÃ©
        if not preserve_sf_data:
            files_to_remove.extend([
                "./exports/tous_les_liens_sortants.csv",
                "./exports/all_outlinks.csv"
            ])
        
        for file_path in files_to_remove:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    print(f"   ğŸ—‘ï¸  SupprimÃ©: {file_path}")
            except Exception as e:
                print(f"   âš ï¸  Erreur suppression {file_path}: {e}")
    
    def intelligent_post_processing(self, website_url: str, section_filter: str = "") -> str:
        """Post-traitement intelligent des rÃ©sultats SF avec l'IA"""
        
        # Chercher le fichier de donnÃ©es SF disponible
        sf_files = [
            "./exports/tous_les_liens_sortants.csv",
            "./exports/all_outlinks.csv"
        ]
        
        sf_links_file = None
        for file_path in sf_files:
            if os.path.exists(file_path):
                sf_links_file = file_path
                break
        
        if not sf_links_file:
            print(f"   âŒ Aucun fichier SF trouvÃ© dans: {', '.join(sf_files)}")
            return ""
        
        print(f"   ğŸ“‚ Chargement des rÃ©sultats SF...")
        
        try:
            with open(sf_links_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                sf_links = list(reader)
            
            print(f"   ğŸ“Š {len(sf_links)} liens trouvÃ©s par Screaming Frog")
            
        except Exception as e:
            print(f"   âŒ Erreur lecture SF: {e}")
            return ""
        
        # Appliquer le filtre de section si spÃ©cifiÃ©
        if section_filter:
            print(f"   ğŸ” Application du filtre de section: {section_filter}")
            sf_links = [link for link in sf_links 
                       if section_filter in link.get('Source', '') or section_filter in link.get('Destination', '')]
            print(f"   ğŸ“Š {len(sf_links)} liens aprÃ¨s filtrage de section")
        
        # Filtrer avec l'intelligence IA
        print(f"   ğŸ¤– Application des filtres IA pour identifier les liens Ã©ditoriaux...")
        
        editorial_links = self.filter_links_with_ai_analysis(sf_links, website_url)
        
        print(f"   âœ… {len(editorial_links)} liens Ã©ditoriaux identifiÃ©s")
        if len(sf_links) > 0:
            print(f"   ğŸ“Š Ratio Ã©ditorial: {len(editorial_links)/len(sf_links)*100:.1f}%")
        else:
            print(f"   âš ï¸  Aucun lien trouvÃ© aprÃ¨s filtrage - vÃ©rifiez le filtre de section")
        
        # GÃ©nÃ©rer le rapport final
        report_path = self.generate_intelligent_report(editorial_links, website_url, len(sf_links))
        
        return report_path
    
    def filter_links_with_ai_analysis(self, sf_links: list, website_url: str) -> list:
        """Filtrer les liens SF avec l'analyse IA avancÃ©e"""
        editorial_links = []
        processed_pages = {}
        
        base_domain = urlparse(website_url).netloc
        
        print(f"   ğŸ” Analyse page par page avec dÃ©tection IA...")
        
        for i, link in enumerate(sf_links):
            source_url = link.get('Source', '')
            dest_url = link.get('Destination', '')
            anchor_text = link.get('Anchor', '') or link.get('Ancrage', '')
            
            # Filtrer les liens internes uniquement
            if not dest_url.startswith(f"http://{base_domain}") and not dest_url.startswith(f"https://{base_domain}"):
                continue
            
            # VÃ©rifier si la page source a dÃ©jÃ  Ã©tÃ© analysÃ©e
            if source_url not in processed_pages:
                page_editorial_links = self.extract_editorial_links_from_page_with_ai(source_url)
                processed_pages[source_url] = page_editorial_links
                
                # Afficher le progrÃ¨s
                if (len(processed_pages)) % 10 == 0:
                    print(f"      ğŸ“Š AnalysÃ© {len(processed_pages)} pages uniques...")
            
            # VÃ©rifier si ce lien spÃ©cifique est Ã©ditorial
            editorial_links_on_page = processed_pages[source_url]
            
            if self.is_link_editorial_advanced(link, editorial_links_on_page, website_url):
                editorial_links.append(link)
        
        return editorial_links
    
    def extract_editorial_links_from_page_with_ai(self, page_url: str) -> list:
        """Extraire les liens Ã©ditoriaux d'une page en utilisant l'analyse IA"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(page_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Utiliser l'analyse IA pour identifier les zones de contenu
            editorial_link_elements = self.find_editorial_links_with_ai_xpath(soup)
            
            # Extraire les informations des liens
            editorial_links = []
            for link_elem in editorial_link_elements:
                href = link_elem.get('href', '')
                text = link_elem.get_text(strip=True)
                
                if href and text and len(text.strip()) > 2:  # Filtrer les liens vides
                    full_url = urljoin(page_url, href)
                    editorial_links.append({
                        'href': full_url,
                        'text': text,
                        'context': self.get_link_context(link_elem)
                    })
            
            return editorial_links
            
        except Exception as e:
            # Ã‰chec silencieux pour Ã©viter de ralentir le processus
            return []
    
    def find_editorial_links_with_ai_xpath(self, soup: BeautifulSoup) -> list:
        """Trouver les liens Ã©ditoriaux en utilisant l'analyse IA"""
        editorial_links = []
        
        # StratÃ©gie 1: Utiliser les zones de contenu identifiÃ©es par l'IA
        content_zones = []
        
        # Rechercher les zones main, article
        main_zones = soup.find_all(['main', 'article'])
        content_zones.extend(main_zones)
        
        # Rechercher les sections de contenu (selon analyse IA)
        if 'div[@class=' in self.xpath_content:
            # Extraire les classes CSS de l'XPath IA
            css_classes = self.extract_css_classes_from_xpath(self.xpath_content)
            for css_class in css_classes:
                sections = soup.find_all('div', class_=css_class)
                content_zones.extend(sections)
        else:
            # Fallback: sections gÃ©nÃ©riques
            sections = soup.find_all('section')
            content_zones.extend(sections)
        
        # Extraire les liens des zones de contenu
        for zone in content_zones:
            if zone:
                links = zone.find_all('a', href=True)
                editorial_links.extend(links)
        
        # Filtrer pour exclure les zones de navigation
        filtered_links = []
        for link in editorial_links:
            if not self.is_in_navigation_zone(link):
                filtered_links.append(link)
        
        return filtered_links
    
    def extract_css_classes_from_xpath(self, xpath: str) -> list:
        """Extraire les classes CSS d'un XPath"""
        classes = []
        if "[@class='" in xpath:
            parts = xpath.split("[@class='")
            for part in parts[1:]:
                class_end = part.find("']")
                if class_end != -1:
                    class_name = part[:class_end]
                    classes.append(class_name)
        return classes
    
    def is_in_navigation_zone(self, link_element) -> bool:
        """VÃ©rifier si un lien est dans une zone de navigation"""
        # Remonter l'arbre DOM pour chercher des zones de navigation
        current = link_element.parent
        
        for _ in range(5):  # Remonter maximum 5 niveaux
            if current is None:
                break
                
            # VÃ©rifier le nom de la balise
            if current.name in ['nav', 'header', 'footer', 'aside']:
                return True
            
            # VÃ©rifier les classes/IDs
            classes = current.get('class', [])
            element_id = current.get('id', '')
            
            nav_indicators = ['nav', 'menu', 'header', 'footer', 'sidebar', 'breadcrumb']
            
            for indicator in nav_indicators:
                if (any(indicator in str(c).lower() for c in classes) or 
                    indicator in element_id.lower()):
                    return True
            
            current = current.parent
        
        return False
    
    def get_link_context(self, link_element) -> str:
        """Obtenir le contexte d'un lien (texte environnant)"""
        try:
            parent = link_element.parent
            if parent:
                context = parent.get_text(strip=True)
                # Limiter la longueur du contexte
                return context[:100] + "..." if len(context) > 100 else context
        except:
            pass
        return ""
    
    def is_link_editorial_advanced(self, sf_link: dict, editorial_links: list, website_url: str) -> bool:
        """VÃ©rification avancÃ©e si un lien SF correspond aux liens Ã©ditoriaux"""
        dest_url = sf_link.get('Destination', '')
        anchor_text = sf_link.get('Anchor', '') or sf_link.get('Ancrage', '')
        
        # Nettoyer l'URL de destination
        dest_url_clean = dest_url.rstrip('/')
        
        # Comparer avec les liens Ã©ditoriaux extraits
        for editorial_link in editorial_links:
            editorial_href_clean = editorial_link['href'].rstrip('/')
            editorial_text = editorial_link['text']
            
            # Correspondance exacte URL + texte
            if (dest_url_clean == editorial_href_clean and 
                anchor_text.strip() == editorial_text.strip()):
                return True
            
            # Correspondance URL avec tolÃ©rance sur le texte
            if (dest_url_clean == editorial_href_clean and
                len(anchor_text.strip()) > 3 and
                anchor_text.strip().lower() in editorial_text.lower()):
                return True
        
        return False
    
    def generate_intelligent_report(self, editorial_links: list, website_url: str, total_links: int) -> str:
        """GÃ©nÃ©rer le rapport final avec les liens Ã©ditoriaux filtrÃ©s"""
        
        output_file = f"./exports/editorial_links_intelligent.csv"
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8') as f:
                if editorial_links:
                    fieldnames = editorial_links[0].keys()
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(editorial_links)
            
            print(f"   ğŸ’¾ Rapport intelligent gÃ©nÃ©rÃ©: {output_file}")
            
            # Statistiques finales
            internal_editorial = [link for link in editorial_links 
                                if link.get('Destination', '').startswith(website_url)]
            
            print(f"   ğŸ“Š STATISTIQUES FINALES:")
            print(f"      ğŸŒ Total liens crawlÃ©s: {total_links}")
            print(f"      âœï¸  Liens Ã©ditoriaux dÃ©tectÃ©s: {len(editorial_links)}")
            print(f"      ğŸ  Liens Ã©ditoriaux internes: {len(internal_editorial)}")
            print(f"      ğŸ“Š Ratio d'efficacitÃ©: {len(editorial_links)/total_links*100:.1f}%")
            
            # Analyser les domaines de destination
            domains = {}
            for link in editorial_links:
                domain = urlparse(link.get('Destination', '')).netloc
                domains[domain] = domains.get(domain, 0) + 1
            
            print(f"      ğŸŒ Top domaines liÃ©s:")
            for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"         â€¢ {domain}: {count} liens")
            
            return output_file
            
        except Exception as e:
            print(f"   âŒ Erreur gÃ©nÃ©ration rapport: {e}")
            return ""


    def _validate_section_filter(self, section_input: str, website_url: str) -> str:
        """Valider et nettoyer le filtre de section"""
        if not section_input:
            return ""

        # Si c'est une URL complÃ¨te, extraire le path
        if section_input.startswith(('http://', 'https://')):
            parsed = urlparse(section_input)
            path = parsed.path.rstrip('/')
            if path:
                print(f"   ğŸ“ URL dÃ©tectÃ©e, utilisation du path: {path}")
                return path

        # Nettoyer le path
        section_filter = section_input.strip()

        # S'assurer qu'il commence par /
        if not section_filter.startswith('/'):
            section_filter = '/' + section_filter

        # Supprimer les trailing slashes
        section_filter = section_filter.rstrip('/')

        # Validation basique
        if len(section_filter) > 1 and section_filter.count('/') <= 3:
            return section_filter
        else:
            print(f"   âš ï¸  Format de section invalide: {section_input}")
            print(f"   ğŸ’¡ Exemples valides: /blog, /produits, /centre-dexpertise")
            return ""

    def launch_semantic_analysis(self, filtered_report_path: str) -> bool:
        """Lancer l'analyse sÃ©mantique avec gÃ©nÃ©ration du rapport HTML complet"""

        if not os.path.exists(filtered_report_path):
            print(f"   âŒ Rapport filtrÃ© introuvable: {filtered_report_path}")
            return False

        print(f"   ğŸš€ Lancement de l'analyse sÃ©mantique complÃ¨te...")
        print(f"   ğŸ“„ Fichier source: {filtered_report_path}")

        try:
            # Import direct de l'analyseur sÃ©mantique pour avoir plus de contrÃ´le
            from ext_audit_maillage_classique import CompleteLinkAuditor

            auditor = CompleteLinkAuditor()

            # Analyser le fichier CSV filtrÃ© avec gÃ©nÃ©ration complÃ¨te du rapport HTML
            print(f"   ğŸ“Š Analyse des donnÃ©es avec CamemBERT...")
            report_path = auditor.analyze_csv(filtered_report_path)

            if report_path:
                print(f"   âœ… Rapport HTML complet gÃ©nÃ©rÃ©: {report_path}")
                print(f"   ğŸ“Š Incluant: graphique des nÅ“uds, clusters sÃ©mantiques, recommandations")
                return True
            else:
                print(f"   âš ï¸  Analyse terminÃ©e mais rapport non gÃ©nÃ©rÃ©")
                return False

        except ImportError:
            print(f"   âš ï¸  Import de l'analyseur sÃ©mantique Ã©chouÃ©, utilisation du subprocess...")
            # Fallback vers l'ancienne mÃ©thode
            try:
                result = subprocess.run([
                    'python', 'ext_analyseur_semantique.py',
                    '--csv-file', filtered_report_path
                ], capture_output=True, text=True, timeout=300)

                if result.returncode == 0:
                    print(f"   âœ… Analyse sÃ©mantique terminÃ©e")
                    return True
                else:
                    print(f"   âš ï¸  Analyse terminÃ©e avec avertissements")
                    return True

            except Exception as e:
                print(f"   âŒ Erreur subprocess: {e}")
                return False

        except Exception as e:
            print(f"   âŒ Erreur analyse sÃ©mantique: {e}")
            print(f"   ğŸ’¡ Vous pouvez lancer manuellement: python ext_audit_maillage_classique.py --csv-file {filtered_report_path}")
            return False
        
        print(f"   ğŸš€ Lancement de l'analyse sÃ©mantique complÃ¨te...")
        print(f"   ğŸ“„ Fichier source: {filtered_report_path}")
        
        try:
            # Import direct de l'analyseur sÃ©mantique pour avoir plus de contrÃ´le
            from ext_audit_maillage_classique import CompleteLinkAuditor
            
            auditor = CompleteLinkAuditor()
            
            # Analyser le fichier CSV filtrÃ© avec gÃ©nÃ©ration complÃ¨te du rapport HTML
            print(f"   ğŸ“Š Analyse des donnÃ©es avec CamemBERT...")
            report_path = auditor.analyze_csv(filtered_report_path)
            
            if report_path:
                print(f"   âœ… Rapport HTML complet gÃ©nÃ©rÃ©: {report_path}")
                print(f"   ğŸ“Š Incluant: graphique des nÅ“uds, clusters sÃ©mantiques, recommandations")
                return True
            else:
                print(f"   âš ï¸  Analyse terminÃ©e mais rapport non gÃ©nÃ©rÃ©")
                return False
                
        except ImportError:
            print(f"   âš ï¸  Import de l'analyseur sÃ©mantique Ã©chouÃ©, utilisation du subprocess...")
            # Fallback vers l'ancienne mÃ©thode
            try:
                result = subprocess.run([
                    'python', 'ext_audit_maillage_classique.py',
                    '--csv-file', filtered_report_path
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    print(f"   âœ… Analyse sÃ©mantique terminÃ©e")
                    return True
                else:
                    print(f"   âš ï¸  Analyse terminÃ©e avec avertissements")
                    return True
                    
            except Exception as e:
                print(f"   âŒ Erreur subprocess: {e}")
                return False
        
        except Exception as e:
            print(f"   âŒ Erreur analyse sÃ©mantique: {e}")
            print(f"   ğŸ’¡ Vous pouvez lancer manuellement: python ext_audit_maillage_classique.py --csv-file {filtered_report_path}")
            return False

def main():
    """Lancement du workflow intelligent final"""
    print("ğŸ¤– AUDIT DE MAILLAGE INTERNE INTELLIGENT")
    print("=" * 80)
    
    workflow = FinalIntelligentWorkflow()
    
    # Menu interactif
    print("\nChoisissez une option:")
    print("1. ğŸš€ Nouveau crawl intelligent (IA + Screaming Frog)")
    print("2. ğŸ“Š Analyser un CSV existant")
    print("3. âš™ï¸  Configuration IA seulement (gÃ©nÃ©rer config SF)")
    print("4. ğŸ“„ Analyser via sitemap XML")
    print("5. âŒ Quitter")
    
    choice = input("\nVotre choix (1-5): ").strip()
    result = None

    if choice == "1":
        website_url = input("ğŸŒ URL du site Ã  analyser: ").strip()
        if not website_url:
            print("âŒ URL requise")
            return

        # Options avancÃ©es
        print("\nğŸ”§ Options avancÃ©es (optionnel):")
        sample_urls_input = input("ğŸ“„ URLs d'exemple pour analyser la structure (sÃ©parÃ©es par des virgules, ou vide pour auto): ").strip()

        sample_urls = []
        if sample_urls_input:
            sample_urls = [url.strip() for url in sample_urls_input.split(',') if url.strip()]
            print(f"   ğŸ“ {len(sample_urls)} URLs fournies pour l'analyse")

        section_filter_raw = input("ğŸ“‚ Analyser seulement une section (ex: /blog/, /produits/): ").strip()

        # Validation et nettoyage du filtre de section
        section_filter = workflow._validate_section_filter(section_filter_raw, website_url)

        max_pages = input("ğŸ“Š Limite de pages (dÃ©faut: illimitÃ©): ").strip()

        print(f"\nğŸš€ Lancement de l'analyse de {website_url}")
        if section_filter:
            print(f"   ğŸ“‚ Section filtrÃ©e: {section_filter}")
        if max_pages:
            print(f"   ğŸ“Š Limite: {max_pages} pages")

        result = workflow.run_complete_workflow(website_url, section_filter, max_pages, sample_urls)
    
    elif choice == "2":
        csv_files = glob.glob("./exports/*.csv")
        if not csv_files:
            print("âŒ Aucun fichier CSV trouvÃ© dans ./exports/")
            return
        
        print("\nğŸ“ Fichiers CSV disponibles:")
        for i, file in enumerate(csv_files, 1):
            print(f"   {i}. {os.path.basename(file)}")
        
        try:
            file_choice = int(input("\nChoisir un fichier (numÃ©ro): ")) - 1
            if 0 <= file_choice < len(csv_files):
                result = workflow.launch_semantic_analysis(csv_files[file_choice])
                if result:
                    print(f"âœ… Analyse terminÃ©e!")
            else:
                print("âŒ Choix invalide")
        except ValueError:
            print("âŒ Veuillez entrer un numÃ©ro valide")
            
    elif choice == "3":
        website_url = input("ğŸŒ URL du site pour analyser la structure: ").strip()
        if not website_url:
            print("âŒ URL requise")
            return
        
        print(f"ğŸ¤– GÃ©nÃ©ration de la configuration IA pour {website_url}...")
        structure_analysis = workflow.detector.run_intelligent_workflow(website_url, "")
        
        if structure_analysis.get('success'):
            print("âœ… Configuration Screaming Frog gÃ©nÃ©rÃ©e: ./sf_content_config.xml")
            print("\nğŸ“‹ Commande Screaming Frog Ã  exÃ©cuter:")
            print(structure_analysis.get('sf_command', 'Commande non disponible'))
        else:
            print("âŒ Ã‰chec de la gÃ©nÃ©ration de configuration")

    elif choice == "4":
        sitemap_url = input("ğŸ“„ URL du sitemap XML: ").strip()
        if not sitemap_url:
            print("âŒ URL du sitemap requise")
            return

        print(f"ğŸ“„ Analyse du sitemap: {sitemap_url}")

        # RÃ©cupÃ©rer et analyser le sitemap
        sitemap_urls = workflow.fetch_and_parse_sitemap(sitemap_url)
        if not sitemap_urls:
            print("âŒ Impossible de rÃ©cupÃ©rer ou analyser le sitemap")
            return

        print(f"   ğŸ“ {len(sitemap_urls)} URLs trouvÃ©es dans le sitemap")

        # Extraire l'URL de base du site
        from urllib.parse import urlparse
        parsed = urlparse(sitemap_url)
        website_url = f"{parsed.scheme}://{parsed.netloc}/"

        print(f"   ğŸŒ Site dÃ©tectÃ©: {website_url}")

        # Lancer l'analyse avec les URLs du sitemap
        result = workflow.run_complete_workflow(website_url, "", "", sitemap_urls)

    elif choice == "5":
        print("ğŸ‘‹ Au revoir!")
        return
    
    else:
        print("âŒ Choix invalide")
        return
    
    if result:
        print(f"\nğŸ‰ SUCCÃˆS TOTAL!")
        print(f"ğŸ“Š Rapport final gÃ©nÃ©rÃ©: {result}")
        print(f"\nğŸ“‹ PROCHAINES Ã‰TAPES:")
        print(f"   1. Examinez le rapport: {result}")
        print(f"   2. Analysez les recommandations sÃ©mantiques")
        print(f"   3. ImplÃ©mentez les amÃ©liorations de maillage interne")
        print(f"\nğŸ’¡ Pour relancer l'analyse sÃ©mantique uniquement:")
        print(f"   python ext_audit_maillage_classique.py --csv-file {result}")
    else:
        print(f"\nâŒ Ã‰CHEC DU WORKFLOW")
        print(f"VÃ©rifiez les messages d'erreur ci-dessus pour diagnostiquer le problÃ¨me.")

if __name__ == "__main__":
    main()